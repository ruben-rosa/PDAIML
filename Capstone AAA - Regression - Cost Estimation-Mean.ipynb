{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of Costs Prediction Model - Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librerias\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "from datetime import datetime\n",
    "import datetime as dt  # Used to manage dates\n",
    "import warnings                   # To ignore the warnings warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor, plot_importance \n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "import scipy.stats as stats \n",
    "from scipy.stats import norm, skew\n",
    "import scikitplot as skplt\n",
    "\n",
    "import plotly.offline as py\n",
    "from plotly import graph_objs as go\n",
    "from plotly import figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning, )\n",
    "\n",
    "%matplotlib inline\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "rcParams.update({'figure.autolayout': True, 'figure.figsize':(12,8),'axes.titlesize':14})\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('max_info_rows', 100)\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, file_name):\n",
    "    # Write data to CSV File\n",
    "    df.to_csv(path_or_buf=file_name, sep=',',index=False, encoding='utf-8')\n",
    "    return\n",
    "\n",
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',', index_col=False, header='infer', float_precision=2 )\n",
    "    return df\n",
    "\n",
    "def draw_col_shape(df_feature,cols):\n",
    "    # draw a bar graph with he number of members of a columns specify in field Cols\n",
    "    \n",
    "    # Dictionary for X Axis Labels\n",
    "    dict_cols = { 'income':'Income', 'member_status':'Member Status','state_grouped': 'State', 'credit_ranges':'Credit Range',\n",
    "                 'cancel_reason':'Cancel Reason','renew_method':'Renewal Method','sc_vehicle_manufacturer_name':'Vehicle Manufacturer',\n",
    "                 'sc_vehicle_model_name':'Vehicle Model','race': 'Ethnic Group','home_owner': 'Home Owner','education':'Education Level', \n",
    "                 'dwelling_type': 'Dwelling Type', 'gender':'Gender','occupation_code':'Occupation', 'occupation_group':'Occupation Group',\n",
    "                 'occupant_type': 'Occupant Type','mosaic_household':'Mosaic household','mosaic_global_household':'Global Mosaic',\n",
    "                 'kcl_b_ind_mosaicsgrouping':'Mosaic Grouping', 'plus_indicator_description':'Membership Type',\n",
    "                 'tenure':'Tenure Range', 'generation':'Generation', 'total_calls':'Total Breakdown Calls', 'aaa_mortgage':'Used Mortgage Service',\n",
    "                  'aaa_credit_card':'Use Credit Card Service','aaa_deposit':'Use Deposit Service','aaa_travel':'Travel Services',\n",
    "                 'aaa_home_equity':'Use Home Equity Service', 'aaa_financial_service':'Use Financial Service', \n",
    "                 'aaa_auto_insurance':'Use Insurance Service', 'vehicle':'Vehicle','total_member_cost':'Total Member Avg. Costs',\n",
    "                 'no_members':'Total Members with Household', 'aaa_id_theft':'ID Theft Service', 'use_road_side':'Use Roadside'\n",
    "                }\n",
    "\n",
    "\n",
    "    # number of members per state\n",
    "\n",
    "    for col in cols:\n",
    "        counts = df_feature[col].value_counts()\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.axhline(0, color=\"k\", clip_on=False)\n",
    "        plt.ylabel(\"# of Classes\",fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Classes\", fontsize=14, fontweight='bold')\n",
    "        plt.title('Total  Number of Classes',loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "        sns.barplot(counts.index, counts.values)\n",
    "        plt.xticks(\n",
    "            rotation=45, \n",
    "            horizontalalignment='right',\n",
    "            fontweight='light',\n",
    "            fontsize='large')\n",
    "        i = 0\n",
    "        for v in list(counts.index):\n",
    "            plt.text(i-.25, counts.values[v], str(counts.values[v]), color='blue', fontweight='bold')\n",
    "            i +=1\n",
    "        plt.show()\n",
    "        counts = pd.DataFrame(df_feature[col].value_counts().nlargest(15), index=None)\n",
    "        counts.reset_index(inplace=True)\n",
    "        counts.rename(columns={\"index\": dict_cols[col], col: \"# of Classes\"}, inplace=True)\n",
    "        fig = ff.create_table(counts, height_constant=30, index=False)\n",
    "        py.iplot(fig)\n",
    "        return\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-10):\n",
    "    # To calculate the cross Entropy Loss to validate model performance\n",
    "    # Cross Entropy Loss\n",
    "        # Cross-entropy loss, or log loss, measures the performance of a classification model \n",
    "        # whose output is a probability value between 0 and 1. #\n",
    "        # Cross-entropy loss increases as the predicted probability diverges from the actual label. \n",
    "        # So predicting a probability of .012 when the actual observation label is 1 would be bad and \n",
    "        # result in a high loss value. # A perfect model would have a log loss of 0.\n",
    "        \n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5)))/N\n",
    "    return ce_loss\n",
    "\n",
    "def road_side_usage(calls):\n",
    "    if calls > 0: return 1\n",
    "    elif calls == 0: return 0\n",
    "    else: return np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from consolidate file by household key and transacton file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2937 entries, 0 to 5260\n",
      "Data columns (total 87 columns):\n",
      "membership_id                        int64\n",
      "household_key                        int64\n",
      "income                               object\n",
      "member_status                        object\n",
      "state_grouped                        object\n",
      "credit_ranges                        object\n",
      "cancel_reason                        object\n",
      "renew_method                         object\n",
      "plus_indicator_description           object\n",
      "zip                                  int64\n",
      "sc_vehicle_manufacturer_name         object\n",
      "sc_vehicle_model_name                object\n",
      "race                                 object\n",
      "home_owner                           object\n",
      "education                            object\n",
      "dwelling_type                        object\n",
      "gender                               object\n",
      "occupation_code                      object\n",
      "occupation_group                     object\n",
      "occupant_type                        object\n",
      "mosaic_household                     object\n",
      "mosaic_global_household              object\n",
      "kcl_b_ind_mosaicsgrouping            object\n",
      "generation                           object\n",
      "tenure                               object\n",
      "aaa_no_tenure                        int64\n",
      "aaa_member_tenure_years              int64\n",
      "aaa_mean_age                         float64\n",
      "length_of_residence                  float64\n",
      "aaa_mortgage                         int64\n",
      "aaa_credit_card                      int64\n",
      "aaa_deposit                          int64\n",
      "aaa_home_equity                      int64\n",
      "aaa_financial_service                int64\n",
      "aaa_auto_insurance                   int64\n",
      "aaa_id_theft                         int64\n",
      "aaa_motorcycle_indicator             float64\n",
      "aaa_travel                           int64\n",
      "aaa_mean_child                       float64\n",
      "aaa_mean_total_cost                  float64\n",
      "aaa_no_race                          float64\n",
      "aaa_no_home_owner                    float64\n",
      "aaa_no_education                     float64\n",
      "aaa_no_income                        float64\n",
      "aaa_no_dwelling_type                 float64\n",
      "aaa_no_credit_ranges                 float64\n",
      "aaa_no_gender                        float64\n",
      "aaa_no_language                      float64\n",
      "aaa_no_reason_joined                 float64\n",
      "aaa_cancel_reason                    float64\n",
      "aaa_no_mosaic_household              float64\n",
      "aaa_no_mosaic_global_household       float64\n",
      "aaa_no_kcl_b_ind_mosaicsgrouping     float64\n",
      "aaa_no_occupation_code               float64\n",
      "aaa_no_occupation_group              float64\n",
      "aaa_no_occupant_type                 float64\n",
      "aaa_no_plus_indicator_description    float64\n",
      "aaa_no_generation                    float64\n",
      "total_calls_year_1                   float64\n",
      "total_calls_year_2                   float64\n",
      "total_calls_year_3                   float64\n",
      "total_member_cost_1                  float64\n",
      "total_member_cost_2                  float64\n",
      "total_member_cost_3                  float64\n",
      "basic_cost                           float64\n",
      "plus_cost                            float64\n",
      "premier_cost                         float64\n",
      "mean_basic_cost                      float64\n",
      "mean_plus_cost                       float64\n",
      "mean_premier_cost                    float64\n",
      "total_calls                          float64\n",
      "total_member_cost                    float64\n",
      "mean_total_member_cost               float64\n",
      "total_tow_miles                      float64\n",
      "mean_tow_miles                       float64\n",
      "vehicle                              object\n",
      "total_cost                           float64\n",
      "mean_total_cost                      float64\n",
      "total_members_in_household           int64\n",
      "total_calls_veh                      float64\n",
      "total_cost_veh                       float64\n",
      "total_member_cost_veh                float64\n",
      "mean_total_calls_veh                 float64\n",
      "mean_total_cost_veh                  float64\n",
      "mean_total_member_cost_veh           float64\n",
      "consumer_score                       float64\n",
      "use_road_side                        int64\n",
      "dtypes: float64(49), int64(15), object(23)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load data from the View create using Capstone AAA - EDA .ipynb\n",
    "df_household = read_data('household_view.csv')\n",
    "df_transaction = read_data('capstone_aaa_featured.csv')\n",
    "df_original_house_hold = df_household.copy()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "var_to_predict = ['mean_total_cost']\n",
    "var_to_predict_title = 'Mean Total Costs Forecast'\n",
    "var_to_predict_save = 'totc'\n",
    "test_size = 0.5\n",
    "# Create roadside usage\n",
    "df_household['use_road_side'] = df_household['total_calls'].apply(lambda x:road_side_usage(x))\n",
    "df_household.consumer_score = np.where(df_household.consumer_score!= np.nan, 1/df_household.consumer_score, \n",
    "                                    df_household.consumer_score)\n",
    "df_household.consumer_score = np.where(df_household.consumer_score== np.nan, df_household.consumer_score.mean(), \n",
    "                                    df_household.consumer_score)\n",
    "# Remove members with status CANCELLED\n",
    "df_household = df_household[df_household.member_status!='CANCELLED']\n",
    "df_household.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis the Total cost series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>membership_id</th>\n",
       "      <th>household_key</th>\n",
       "      <th>income</th>\n",
       "      <th>member_status</th>\n",
       "      <th>state_grouped</th>\n",
       "      <th>credit_ranges</th>\n",
       "      <th>cancel_reason</th>\n",
       "      <th>renew_method</th>\n",
       "      <th>plus_indicator_description</th>\n",
       "      <th>zip</th>\n",
       "      <th>sc_vehicle_manufacturer_name</th>\n",
       "      <th>sc_vehicle_model_name</th>\n",
       "      <th>race</th>\n",
       "      <th>home_owner</th>\n",
       "      <th>education</th>\n",
       "      <th>dwelling_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>occupation_group</th>\n",
       "      <th>occupant_type</th>\n",
       "      <th>mosaic_household</th>\n",
       "      <th>mosaic_global_household</th>\n",
       "      <th>kcl_b_ind_mosaicsgrouping</th>\n",
       "      <th>generation</th>\n",
       "      <th>tenure</th>\n",
       "      <th>aaa_no_tenure</th>\n",
       "      <th>aaa_member_tenure_years</th>\n",
       "      <th>aaa_mean_age</th>\n",
       "      <th>length_of_residence</th>\n",
       "      <th>aaa_mortgage</th>\n",
       "      <th>aaa_credit_card</th>\n",
       "      <th>aaa_deposit</th>\n",
       "      <th>aaa_home_equity</th>\n",
       "      <th>aaa_financial_service</th>\n",
       "      <th>aaa_auto_insurance</th>\n",
       "      <th>aaa_id_theft</th>\n",
       "      <th>aaa_motorcycle_indicator</th>\n",
       "      <th>aaa_travel</th>\n",
       "      <th>aaa_mean_child</th>\n",
       "      <th>aaa_mean_total_cost</th>\n",
       "      <th>aaa_no_race</th>\n",
       "      <th>aaa_no_home_owner</th>\n",
       "      <th>aaa_no_education</th>\n",
       "      <th>aaa_no_income</th>\n",
       "      <th>aaa_no_dwelling_type</th>\n",
       "      <th>aaa_no_credit_ranges</th>\n",
       "      <th>aaa_no_gender</th>\n",
       "      <th>aaa_no_language</th>\n",
       "      <th>aaa_no_reason_joined</th>\n",
       "      <th>aaa_cancel_reason</th>\n",
       "      <th>aaa_no_mosaic_household</th>\n",
       "      <th>aaa_no_mosaic_global_household</th>\n",
       "      <th>aaa_no_kcl_b_ind_mosaicsgrouping</th>\n",
       "      <th>aaa_no_occupation_code</th>\n",
       "      <th>aaa_no_occupation_group</th>\n",
       "      <th>aaa_no_occupant_type</th>\n",
       "      <th>aaa_no_plus_indicator_description</th>\n",
       "      <th>aaa_no_generation</th>\n",
       "      <th>total_calls_year_1</th>\n",
       "      <th>total_calls_year_2</th>\n",
       "      <th>total_calls_year_3</th>\n",
       "      <th>total_member_cost_1</th>\n",
       "      <th>total_member_cost_2</th>\n",
       "      <th>total_member_cost_3</th>\n",
       "      <th>basic_cost</th>\n",
       "      <th>plus_cost</th>\n",
       "      <th>premier_cost</th>\n",
       "      <th>mean_basic_cost</th>\n",
       "      <th>mean_plus_cost</th>\n",
       "      <th>mean_premier_cost</th>\n",
       "      <th>total_calls</th>\n",
       "      <th>total_member_cost</th>\n",
       "      <th>mean_total_member_cost</th>\n",
       "      <th>total_tow_miles</th>\n",
       "      <th>mean_tow_miles</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>mean_total_cost</th>\n",
       "      <th>total_members_in_household</th>\n",
       "      <th>total_calls_veh</th>\n",
       "      <th>total_cost_veh</th>\n",
       "      <th>total_member_cost_veh</th>\n",
       "      <th>mean_total_calls_veh</th>\n",
       "      <th>mean_total_cost_veh</th>\n",
       "      <th>mean_total_member_cost_veh</th>\n",
       "      <th>consumer_score</th>\n",
       "      <th>use_road_side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>4500791</td>\n",
       "      <td>30-39,999</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>RI</td>\n",
       "      <td>600-649</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>AUTO RENEW</td>\n",
       "      <td>BASIC MEMBERSHIP</td>\n",
       "      <td>2893</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>CAMRY</td>\n",
       "      <td>CAUCASION / WHITE - ENGLISH</td>\n",
       "      <td>HOME OWNER</td>\n",
       "      <td>SOME COLLEGE</td>\n",
       "      <td>SMALL OR LARGE MULTI-FAMILY W/APT NUMBER</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>SENIOR DISCOUNTS</td>\n",
       "      <td>LOW INCOME ELDERS</td>\n",
       "      <td>GOLDEN YEAR GUARDIANS</td>\n",
       "      <td>POST-WAR (&lt; 1948)</td>\n",
       "      <td>+40 YEARS</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>97.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>33.8333</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>585.0000</td>\n",
       "      <td>304.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>33.8333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>585.0000</td>\n",
       "      <td>65.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>TOYOTA CAMRY</td>\n",
       "      <td>304.5000</td>\n",
       "      <td>33.8333</td>\n",
       "      <td>1</td>\n",
       "      <td>5836.0000</td>\n",
       "      <td>107732.6600</td>\n",
       "      <td>303128.5500</td>\n",
       "      <td>2.5485</td>\n",
       "      <td>47.0448</td>\n",
       "      <td>132.3705</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>11622991</td>\n",
       "      <td>50-59,999</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>RI</td>\n",
       "      <td>750-799</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>AUTO RENEW</td>\n",
       "      <td>BASIC MEMBERSHIP</td>\n",
       "      <td>2889</td>\n",
       "      <td>SUBARU</td>\n",
       "      <td>BRZ</td>\n",
       "      <td>HISPANIC - HISPANIC ORIGIN</td>\n",
       "      <td>HOME OWNER</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>SFDU</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NO PLACE LIKE HOME</td>\n",
       "      <td>ROUTINE SERVICE WORKERS</td>\n",
       "      <td>THRIVING BOOMERS</td>\n",
       "      <td>MILLENIALS (1981 - 1993)</td>\n",
       "      <td>BETWEEN 1 &amp; 5 YEARS</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "      <td>47.6667</td>\n",
       "      <td>8.5556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>39.2056</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>176.5500</td>\n",
       "      <td>353.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>352.8500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>39.2056</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>529.6500</td>\n",
       "      <td>58.8500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>SUBARU BRZ</td>\n",
       "      <td>352.8500</td>\n",
       "      <td>39.2056</td>\n",
       "      <td>4</td>\n",
       "      <td>1153.0000</td>\n",
       "      <td>22582.1100</td>\n",
       "      <td>56317.2900</td>\n",
       "      <td>2.3971</td>\n",
       "      <td>46.9483</td>\n",
       "      <td>117.0838</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>182</td>\n",
       "      <td>579810</td>\n",
       "      <td>50-59,999</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>RI</td>\n",
       "      <td>650-699</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NOTICE</td>\n",
       "      <td>BASIC MEMBERSHIP</td>\n",
       "      <td>2863</td>\n",
       "      <td>INFINITI</td>\n",
       "      <td>QX56</td>\n",
       "      <td>CAUCASION / WHITE - EUROPEAN</td>\n",
       "      <td>HOME OWNER</td>\n",
       "      <td>SOME COLLEGE</td>\n",
       "      <td>SFDU</td>\n",
       "      <td>MALE</td>\n",
       "      <td>RETIRED</td>\n",
       "      <td>RETIRED - KNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>TOWN ELDERS</td>\n",
       "      <td>COMFORTABLE RETIREMENT</td>\n",
       "      <td>GOLDEN YEAR GUARDIANS</td>\n",
       "      <td>POST-WAR (&lt; 1948)</td>\n",
       "      <td>+40 YEARS</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>83.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>INFINITI QX56</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>337.0000</td>\n",
       "      <td>4289.6000</td>\n",
       "      <td>18754.4000</td>\n",
       "      <td>3.7444</td>\n",
       "      <td>47.6622</td>\n",
       "      <td>208.3822</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>188</td>\n",
       "      <td>7187017</td>\n",
       "      <td>100-149,999</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>RI</td>\n",
       "      <td>650-699</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NOTICE</td>\n",
       "      <td>BASIC MEMBERSHIP</td>\n",
       "      <td>2888</td>\n",
       "      <td>CHRYSLER</td>\n",
       "      <td>SEBRING</td>\n",
       "      <td>CAUCASION / WHITE - EUROPEAN</td>\n",
       "      <td>HOME OWNER</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>SFDU</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>RETIRED - INFERRED</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>AGING IN PLACE</td>\n",
       "      <td>COMFORTABLE RETIREMENT</td>\n",
       "      <td>AUTUMN YEARS</td>\n",
       "      <td>POST-WAR (&lt; 1948)</td>\n",
       "      <td>BETWEEN 1 &amp; 5 YEARS</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>63.1667</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>53.9750</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.8500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>323.8500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>53.9750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>58.8500</td>\n",
       "      <td>9.8083</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1.1667</td>\n",
       "      <td>CHRYSLER SEBRING</td>\n",
       "      <td>323.8500</td>\n",
       "      <td>53.9750</td>\n",
       "      <td>3</td>\n",
       "      <td>1253.0000</td>\n",
       "      <td>19114.2400</td>\n",
       "      <td>68772.9200</td>\n",
       "      <td>3.1964</td>\n",
       "      <td>48.7608</td>\n",
       "      <td>175.4411</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>7728088</td>\n",
       "      <td>100-149,999</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>RI</td>\n",
       "      <td>800+</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NOTICE</td>\n",
       "      <td>BASIC MEMBERSHIP</td>\n",
       "      <td>2806</td>\n",
       "      <td>LEXUS</td>\n",
       "      <td>CT200H</td>\n",
       "      <td>MIDDLE EASTERN - ARAB</td>\n",
       "      <td>HOME OWNER</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>SFDU</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>RETIRED - INFERRED</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>AMERICAN ROYALTY</td>\n",
       "      <td>BOURGEOIS PROSPERITY</td>\n",
       "      <td>POWER ELITE</td>\n",
       "      <td>BABY BOOMERS (1948 - 1968)</td>\n",
       "      <td>+40 YEARS</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>63.7500</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>39.7500</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.8500</td>\n",
       "      <td>159.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>39.7500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>58.8500</td>\n",
       "      <td>14.7125</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1.7500</td>\n",
       "      <td>LEXUS CT200H</td>\n",
       "      <td>159.0000</td>\n",
       "      <td>39.7500</td>\n",
       "      <td>3</td>\n",
       "      <td>968.0000</td>\n",
       "      <td>15132.4000</td>\n",
       "      <td>49567.9900</td>\n",
       "      <td>2.9785</td>\n",
       "      <td>46.5612</td>\n",
       "      <td>152.5169</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   membership_id  household_key       income member_status state_grouped credit_ranges cancel_reason renew_method plus_indicator_description   zip sc_vehicle_manufacturer_name sc_vehicle_model_name                          race  home_owner     education                             dwelling_type   gender occupation_code    occupation_group occupant_type    mosaic_household  mosaic_global_household kcl_b_ind_mosaicsgrouping                  generation               tenure  aaa_no_tenure  aaa_member_tenure_years  aaa_mean_age  length_of_residence  aaa_mortgage  aaa_credit_card  aaa_deposit  aaa_home_equity  aaa_financial_service  aaa_auto_insurance  aaa_id_theft  aaa_motorcycle_indicator  aaa_travel  aaa_mean_child  aaa_mean_total_cost  aaa_no_race  aaa_no_home_owner  aaa_no_education  aaa_no_income  aaa_no_dwelling_type  aaa_no_credit_ranges  aaa_no_gender  aaa_no_language  aaa_no_reason_joined  aaa_cancel_reason  aaa_no_mosaic_household  aaa_no_mosaic_global_household  \\\n",
       "0            153        4500791    30-39,999        ACTIVE            RI       600-649       UNKNOWN   AUTO RENEW           BASIC MEMBERSHIP  2893                       TOYOTA                 CAMRY   CAUCASION / WHITE - ENGLISH  HOME OWNER  SOME COLLEGE  SMALL OR LARGE MULTI-FAMILY W/APT NUMBER  UNKNOWN         UNKNOWN             UNKNOWN       UNKNOWN    SENIOR DISCOUNTS        LOW INCOME ELDERS     GOLDEN YEAR GUARDIANS           POST-WAR (< 1948)            +40 YEARS              0                       49       97.0000              15.0000             0                1            0                0                      0                   0             0                    0.0000           0          2.0000              33.8333       2.0000             0.0000            1.0000         8.0000                2.0000                5.0000            nan           0.0000                   nan                nan                  11.0000                          7.0000   \n",
       "2            163       11622991    50-59,999        ACTIVE            RI       750-799       UNKNOWN   AUTO RENEW           BASIC MEMBERSHIP  2889                       SUBARU                   BRZ    HISPANIC - HISPANIC ORIGIN  HOME OWNER       UNKNOWN                                      SFDU  UNKNOWN         UNKNOWN             UNKNOWN       UNKNOWN  NO PLACE LIKE HOME  ROUTINE SERVICE WORKERS          THRIVING BOOMERS    MILLENIALS (1981 - 1993)  BETWEEN 1 & 5 YEARS              6                       59       47.6667               8.5556             0                0            0                0                      0                   0             0                    1.0000           0          1.0000              39.2056       6.0000             0.0000            2.0000         4.0000                0.0000                1.0000            nan           0.0000                   nan             1.0000                   3.0000                          3.0000   \n",
       "3            182         579810    50-59,999        ACTIVE            RI       650-699       UNKNOWN       NOTICE           BASIC MEMBERSHIP  2863                     INFINITI                  QX56  CAUCASION / WHITE - EUROPEAN  HOME OWNER  SOME COLLEGE                                      SFDU     MALE         RETIRED     RETIRED - KNOWN       UNKNOWN         TOWN ELDERS   COMFORTABLE RETIREMENT     GOLDEN YEAR GUARDIANS           POST-WAR (< 1948)            +40 YEARS              0                       59       83.0000              15.0000             0                0            0                0                      0                   1             0                    0.0000           0          1.0000              28.0000       1.0000             0.0000            1.0000         4.0000                0.0000                3.0000         2.0000           0.0000                   nan                nan                   6.0000                          1.0000   \n",
       "4            188        7187017  100-149,999        ACTIVE            RI       650-699       UNKNOWN       NOTICE           BASIC MEMBERSHIP  2888                     CHRYSLER               SEBRING  CAUCASION / WHITE - EUROPEAN  HOME OWNER       UNKNOWN                                      SFDU   FEMALE         UNKNOWN  RETIRED - INFERRED       UNKNOWN      AGING IN PLACE   COMFORTABLE RETIREMENT              AUTUMN YEARS           POST-WAR (< 1948)  BETWEEN 1 & 5 YEARS              6                       49       63.1667              15.0000             0                0            0                0                      0                   0             0                    1.0000           1          2.0000              53.9750       1.0000             0.0000               nan         1.0000                0.0000                3.0000         1.0000           0.0000                   nan                nan                   1.0000                          1.0000   \n",
       "5            191        7728088  100-149,999        ACTIVE            RI          800+       UNKNOWN       NOTICE           BASIC MEMBERSHIP  2806                        LEXUS                CT200H         MIDDLE EASTERN - ARAB  HOME OWNER       UNKNOWN                                      SFDU   FEMALE         UNKNOWN  RETIRED - INFERRED       UNKNOWN    AMERICAN ROYALTY     BOURGEOIS PROSPERITY               POWER ELITE  BABY BOOMERS (1948 - 1968)            +40 YEARS              0                       59       63.7500              13.0000             0                0            0                0                      0                   0             0                    0.0000           0          4.5000              39.7500       5.0000             0.0000            1.0000         1.0000                0.0000                4.0000         1.0000           0.0000                   nan                nan                  18.0000                          2.0000   \n",
       "\n",
       "   aaa_no_kcl_b_ind_mosaicsgrouping  aaa_no_occupation_code  aaa_no_occupation_group  aaa_no_occupant_type  aaa_no_plus_indicator_description  aaa_no_generation  total_calls_year_1  total_calls_year_2  total_calls_year_3  total_member_cost_1  total_member_cost_2  total_member_cost_3  basic_cost  plus_cost  premier_cost  mean_basic_cost  mean_plus_cost  mean_premier_cost  total_calls  total_member_cost  mean_total_member_cost  total_tow_miles  mean_tow_miles           vehicle  total_cost  mean_total_cost  total_members_in_household  total_calls_veh  total_cost_veh  total_member_cost_veh  mean_total_calls_veh  mean_total_cost_veh  mean_total_member_cost_veh  consumer_score  use_road_side  \n",
       "0                            3.0000                     nan                      nan                   nan                             0.0000             0.0000              0.0000              0.0000             18.0000               0.0000               0.0000             585.0000    304.5000     0.0000        0.0000          33.8333          0.0000             0.0000      18.0000           585.0000                 65.0000           0.8000          0.0889      TOYOTA CAMRY    304.5000          33.8333                           1        5836.0000     107732.6600            303128.5500                2.5485              47.0448                    132.3705          0.0137              1  \n",
       "2                            4.0000                     nan                      nan                   nan                             2.0000             3.0000              3.0000              6.0000              0.0000             176.5500             353.1000               0.0000    352.8500     0.0000        0.0000          39.2056          0.0000             0.0000       9.0000           529.6500                 58.8500           0.0000          0.0000        SUBARU BRZ    352.8500          39.2056                           4        1153.0000      22582.1100             56317.2900                2.3971              46.9483                    117.0838          0.0123              1  \n",
       "3                            3.0000                  1.0000                   3.0000                   nan                             0.0000             0.0000              0.0000              0.0000              0.0000               0.0000               0.0000               0.0000     28.0000     0.0000        0.0000          28.0000          0.0000             0.0000       0.0000             0.0000                  0.0000           0.0000          0.0000     INFINITI QX56     28.0000          28.0000                           1         337.0000       4289.6000             18754.4000                3.7444              47.6622                    208.3822          0.0143              0  \n",
       "4                            1.0000                     nan                   9.0000                   nan                             0.0000             1.0000              1.0000              0.0000              0.0000              58.8500               0.0000               0.0000    323.8500     0.0000        0.0000          53.9750          0.0000             0.0000       1.0000            58.8500                  9.8083           7.0000          1.1667  CHRYSLER SEBRING    323.8500          53.9750                           3        1253.0000      19114.2400             68772.9200                3.1964              48.7608                    175.4411          0.0156              1  \n",
       "5                            7.0000                     nan                   6.0000                   nan                             0.0000             3.0000              0.0000              0.0000              1.0000               0.0000               0.0000              58.8500    159.0000     0.0000        0.0000          39.7500          0.0000             0.0000       1.0000            58.8500                 14.7125           7.0000          1.7500      LEXUS CT200H    159.0000          39.7500                           3         968.0000      15132.4000             49567.9900                2.9785              46.5612                    152.5169          0.0132              1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_household.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   2937.0000\n",
       "mean     164.2954\n",
       "std      205.5951\n",
       "min        0.0000\n",
       "25%       30.0000\n",
       "50%      106.0000\n",
       "75%      223.7000\n",
       "max     1763.7000\n",
       "Name: total_cost, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_household[var_to_predict[0]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Normal Series')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logaritmic\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "\n",
    "df[var_to_predict[0]] = np.log1p(df[var_to_predict[0]])\n",
    "\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Logaritmic')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Standard Scaler')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MinMax Scaler\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - MinMax Scaler')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Robust Scaler\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = RobustScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Robust Scaler')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Power Transformer\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = PowerTransformer()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Power Transformer')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalizer Transformer\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = Normalizer()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Normalizer')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Max Abs transformer\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = MaxAbsScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Max Abs Scaler')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quantile Transformer Scaler\n",
    "\n",
    "df = pd.DataFrame(df_household,columns=var_to_predict)\n",
    "scaler = QuantileTransformer(1000,'uniform')\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "\n",
    "df = scaled_df.copy()\n",
    "sns.distplot(df[var_to_predict[0]] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df[var_to_predict[0]])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Total Cost Distribution - Quantile Transformer Scaler')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[var_to_predict[0]], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cars that are most likely to break down \n",
    "https://247wallst.com/special-report/2019/12/12/these-new-cars-are-most-likely-to-break-down/2/\n",
    "\n",
    "# Top Car 35 Car brand according to consumer reports\n",
    "https://www.autonews.com/retail/subaru-reaches-top-consumer-reports-rankings-tesla-falters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_veh_manuf = df_transaction.groupby(by='sc_vehicle_manufacturer_name').agg(({'total_calls':sum, \n",
    "                                                                                     'total_cost' :['sum','mean']\n",
    "                                                                                    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_household.mean_total_cost.hist(figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gender\n",
    "fig = px.scatter(df_household, x=var_to_predict[0], y=\"total_calls\", marginal_x=\"box\", \n",
    "                 marginal_y=\"box\", trendline=\"ols\", facet_col=\"gender\",color=\"gender\",\n",
    "                 color_continuous_scale=px.colors.sequential.Viridis, render_mode=\"webgl\"\n",
    "                )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gender\n",
    "fig = px.scatter(df_household, x=var_to_predict[0], y=\"total_calls\", marginal_x=\"box\", \n",
    "                 marginal_y=\"box\", trendline=\"ols\", color=\"tenure\",facet_col=\"gender\",\n",
    "                 color_continuous_scale=px.colors.sequential.Viridis, render_mode=\"webgl\"\n",
    "                )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_transaction.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.groupby(by=var_to_predict[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_household, x=var_to_predict[0], y=\"total_calls\", size=\"total_calls\", color=\"generation\",\n",
    "           hover_name=\"plus_indicator_description\", log_x=True, size_max=60)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_household, x='income', y=var_to_predict[0],\n",
    "             hover_data=[\"plus_indicator_description\", 'gender'], color=\"plus_indicator_description\",\n",
    "             labels={'total_calls':'Total Roadside Calls'}, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of members in household key\n",
    "df_household.total_members_in_household.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "The frequency of id theft service use depend heavily on the number of members within a household.\n",
    "Will consolidate income in five categories: \n",
    "    * a) 1 member within a household \n",
    "    * b) 2 members within a household\n",
    "    * c) 3 members within a household\n",
    "    * b) 4 members within a household\n",
    "    * c) +5 members within a household "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Income\n",
    "df_household.income.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "The frequency of id theft Service use depend heavvily on income. \n",
    "Will consolidate income in three categories: \n",
    "    * a) under 10k - 39999: Under 39.999\n",
    "    * b) 40 - 99,999 : 40 - 99,999\n",
    "    * c) 100k and above : 100+\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['UNDER 10K', '10-19,999','20-29,999', '30-39,999']\n",
    "group2 = ['40-49,999', '50-59,999', '60-69,999', '70-79,999', '80-89,999', '90-99,999']\n",
    "group3 = ['100-149,999', '150 - 174,999', '175 - 199,999', '200 - 249,999', '250K+' ]\n",
    "for i in group1:\n",
    "    df_household['income']=np.where(df_household['income']== i, 'UNDER 39,999', df_household['income'])\n",
    "for i in group2:\n",
    "    df_household['income']=np.where(df_household['income']== i, '40 - 99,999', df_household['income'])\n",
    "for i in group3:\n",
    "    df_household['income']=np.where(df_household['income']== i, '100k+', df_household['income'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenures\n",
    "df_household.tenure.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "The frequency of id theft Service use depend heavily on tenures. Will consolidate tenure in three categories: \n",
    "    * a) above 40 years  \n",
    "    * b) between 21 - 40 \n",
    "    * c) Less 21 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['BETWEEN 6 & 10 YEARS', 'BETWEEN 11 & 15 YEARS', \n",
    "          'BETWEEN 16 & 20 YEARS', 'BETWEEN 1 & 5 YEARS', '< 1 YEAR']\n",
    "group2 = ['BETWEEN 31 & 40 YEARS', 'BETWEEN 21 & 30 YEARS']\n",
    "group3 = ['+40 YEARS']\n",
    "for i in group1:\n",
    "    df_household['tenure']=np.where(df_household['tenure']== i, 'UNDER 20 YEARS', df_household['tenure'])\n",
    "for i in group2:\n",
    "    df_household['tenure']=np.where(df_household['tenure']== i, 'BETWEEN 21 & 40 YEARS', df_household['tenure'])\n",
    "for i in group3:\n",
    "    df_household['tenure']=np.where(df_household['tenure']== i, '+40 YEARS', df_household['tenure'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Ranges\n",
    "df_household.credit_ranges.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "The frequency of id theft Service use depend heavily on credit_ranges. Will consolidate tenure in three categories: \n",
    "    * a) above 800 years \n",
    "    * b) between 700 - 799 \n",
    "    * c) Below 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['499 & LESS', '500-549','550-599','600-649', '650-699']\n",
    "group2 = ['700-749', '750-799']\n",
    "group3 = ['800+']\n",
    "for i in group1:\n",
    "    df_household['credit_ranges']=np.where(df_household['credit_ranges']== i, 'BELOW 700', df_household['credit_ranges'])\n",
    "for i in group2:\n",
    "    df_household['credit_ranges']=np.where(df_household['credit_ranges']== i, 'BETWEEN 700 & 799', df_household['credit_ranges'])\n",
    "for i in group3:\n",
    "    df_household['credit_ranges']=np.where(df_household['credit_ranges']== i, '800+', df_household['credit_ranges'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education\n",
    "df_household.education.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "The frequency of id theft Service use depend heavily on education. Will consolidate graduate with completed collegue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# graph education\n",
    "df_household['education']=np.where(df_household['education'] =='GRADUATED SCHOOL', \n",
    "                                   'COMPLETED COLLEGE', df_household['education']\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation\n",
    "df_household.generation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Race\n",
    "df_household.race.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "The frequency of id theft Service use depend heavily on race being caucasion- white group the predominant. \n",
    "I Will consolidate them in more similar groups\n",
    "    * a) Caucasion - White\n",
    "    * b) Hispanic\n",
    "    * c) Asian\n",
    "    * d) Arabs\n",
    "    * e) America Africans / Indians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['CAUCASION / WHITE - EUROPEAN', 'CAUCASION / WHITE - ENGLISH', 'CAUCASION / WHITE - WHITE NON-AMERICAN',\n",
    "          'CAUCASION / WHITE - UNKNOWN', 'CAUCASION / WHITE - EASTERN EUROPEAN', 'CAUCASION / WHITE - JEWISH',\n",
    "          'CAUCASION / WHITE - GREEK', 'CAUCASION / WHITE - DUTCH'\n",
    "         ]\n",
    "group2 = ['HISPANIC - HISPANIC ORIGIN', 'HISPANIC - PORTUGUESE ORIGIN']\n",
    "group3 = ['ASIAN - CHINESE', 'ASIAN - UNKNOWN', 'ASIAN - JAPANESE', 'ASIAN - INDIAN', 'ASIAN - ORIENTAL']\n",
    "group4 = ['MIDDLE EASTERN - ARAB', 'MIDDLE EASTERN - EGYPTIAN']\n",
    "group5 = ['AFRICAN AMERICAN - AFRICAN ORIGIN', 'AMERICAN INDIAN']\n",
    "\n",
    "for i in group1:\n",
    "    df_household['race']=np.where(df_household['race']== i, 'CAUCASION / WHITE', df_household['race'])\n",
    "for i in group2:\n",
    "    df_household['race']=np.where(df_household['race']== i, 'HISPANIC', df_household['race'])\n",
    "for i in group3:\n",
    "    df_household['race']=np.where(df_household['race']== i, 'ASIAN', df_household['race'])\n",
    "for i in group4:\n",
    "    df_household['race']=np.where(df_household['race']== i, 'ARABS', df_household['race'])\n",
    "for i in group5:\n",
    "    df_household['race']=np.where(df_household['race']== i, 'AMERICAN AFRICANS / INDIANS', df_household['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Owner\n",
    "df_household.home_owner.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "The frequency of id theft Service use depend heavily on home_owner.  Will leave consolidate probable renter with RENTER and probable home owner with home owner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Home Owner\n",
    "df_household['home_owner']=np.where(df_household['home_owner']== 'PROBABLE RENTER', 'RENTER', df_household['home_owner'])\n",
    "df_household['home_owner']=np.where(df_household['home_owner']== 'PROBABLE HOME OWNER', 'HOME OWNER', df_household['home_owner'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cancel reason\n",
    "df_household.cancel_reason.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "This Cancel Reason feature does not seems to be important to weather member choose or not id theft. Even though, for instance those member that their suscription was cancelled or CC rejected, there might be an oportunity to get a CC if credit is available to them.  Will consolidate the reason in three main groups:\n",
    "* a) PAYMENT ISSUES\n",
    "* b) MEMBER ISSUES\n",
    "* c) DATA/AAA ISSUES\n",
    "\n",
    "UKNOWN will be replaced by 'NOT CANCELLED'  It is the largest group and a potential to customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['NON-PAYMENT', 'ERS DECLINE CC', 'BAD ADD SET BY PO ACS', '*BAD CHECKS*', \n",
    "         ]\n",
    "group2 = ['DECEASED', 'NO LONGER DRIVING', 'MEMBERS REQUEST', 'CANCELLED DO NOT RENEW', 'TRANSFER MEMBER',\n",
    "          'DO NOT RENEW', 'MOVED OUT OF TERR', 'TRANSFER MEMBER SUSPEND', 'CANCELLED MBRS REQUEST',\n",
    "          'HEALTH ISSUE', 'NONCOMPLIANT AGE', 'PO RETURN'\n",
    "         ]\n",
    "group3 = ['DATA PROBLEM','DUP MEMBERSHIP','ERS CNCL MSHIP PROGRAM','MANAGEMENT REQUEST', 'CANCEL PROCESS - FUTURE CANCEL DNR']\n",
    "\n",
    "for i in group1:\n",
    "    df_household['cancel_reason']=np.where(df_household['cancel_reason']== i, 'PAYMENT ISSUES', df_household['cancel_reason'])\n",
    "for i in group2:\n",
    "    df_household['cancel_reason']=np.where(df_household['cancel_reason']== i, 'MEMBER ISSUES', df_household['cancel_reason'])\n",
    "for i in group3:\n",
    "    df_household['cancel_reason']=np.where(df_household['cancel_reason']== i, 'DATA/AAA ISSUES', df_household['cancel_reason'])\n",
    "\n",
    "df_household['cancel_reason']=np.where(df_household['cancel_reason']== 'UNKNOWN', 'NOT CANCELLED', df_household['cancel_reason'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mosaic_household\n",
    "df_household.mosaic_household.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mosaic_household\n",
    "df_household.mosaic_global_household.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Reviewing the graph above, these three groups can be consolidated in:\n",
    "* a) METROPOLITAN STRUGGLERS into ROUTINE SERVICE WORKERS\n",
    "* b) SOPHISTICATED SINGLES into BOURGEOIS PROSPERITY\n",
    "* c) RURAL INHERITANCE into CAREER AND FAMILY\n",
    "* d) POST INDUSTRIAL SURVIVORS into ROUTINE SERVICE WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'METROPOLITAN STRUGGLERS', \n",
    "                                                 'ROUTINE SERVICE WORKERS', df_household['mosaic_global_household']\n",
    "                                                )\n",
    "\n",
    "df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'SOPHISTICATED SINGLES', \n",
    "                                                 'BOURGEOIS PROSPERITY', df_household['mosaic_global_household']\n",
    "                                                )\n",
    "\n",
    "df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'RURAL INHERITANCE', \n",
    "                                                 'CAREER AND FAMILY', df_household['mosaic_global_household']\n",
    "                                                )\n",
    "df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'POST INDUSTRIAL SURVIVORS', \n",
    "                                                 'ROUTINE SERVICE WORKERS', df_household['mosaic_global_household']\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kcl_b_ind_mosaicsgrouping\n",
    "df_household.kcl_b_ind_mosaicsgrouping.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Reviewing the graph above, there are five dominant group here\n",
    "* a) GOLDEN YEARS - retired people\n",
    "* b) BOOMING - well established families\n",
    "* c) POWER ELITE - powerufull wealth people\n",
    "* d) FAMILY - middle class families\n",
    "* e) Singles - singles or single moms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['AUTUMN YEARS', 'GOLDEN YEAR GUARDIANS']\n",
    "group2 = ['BOOMING WITH CONFIDENCE', 'THRIVING BOOMERS', 'BLUE SKY BOOMERS']\n",
    "group3 = ['POWER ELITE', 'SURBURBAN STYLE', 'FLOURISHING FAMILIES']\n",
    "group4 = ['FAMILY UNION', 'MIDDLE CLASS MELTING POT', 'PROMISING FAMILIES', 'ECONOMIC CHALLENGES', 'FAMILIES IN MOTION',\n",
    "          'PASTORAL PRIDE']\n",
    "group5 = ['SINGLES AND STARTERS', 'SIGNIFICANT SINGLES', 'YOUNG CITY SOLOS', 'CULTURAL CONNECTIONS',\n",
    "         'ASPIRATIONAL FUSION']\n",
    "for i in group1:\n",
    "    df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'GOLDEN YEAR', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "for i in group2:\n",
    "    df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'BOOMERS', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "for i in group3:\n",
    "    df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'POWER ELITE', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "for i in group4:\n",
    "    df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'MIDDLE CLASS MELTING POT', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "for i in group5:\n",
    "    df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'SINGLES AND STARTERS', df_household['kcl_b_ind_mosaicsgrouping'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vehicle Manufacturer\n",
    "df_household.sc_vehicle_manufacturer_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Reviewing the graph above, there are four main group of car.  \n",
    "\n",
    "* a) Standard for small families, and young professional, \n",
    "* b) Middle End - Middle class families and middle management\n",
    "* c) High End - Success and upper management or people above 55 years no debts and no kids at school\n",
    "\n",
    "Going to group the vehicle manufacturer in those three groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = [ 'TOYOTA', 'FORD', 'HONDA', 'CHEVROLET', 'HYUNDAI', 'NISSAN', 'BUICK', 'SUBARU', 'SUZUKI',\n",
    "         'DODGE', 'CHRYSLER', 'VOLKSWAGEN','KIA', 'MAZDA', 'SATURN', 'PONTIAC', 'SCION', 'MITSUBISHI',\n",
    "          'GEO', 'CRYSLER', 'MG', 'FIAT', 'BICYCLE', 'ISUZU']\n",
    "group2 = ['JEEP', 'MERCURY','GMC', 'OLSMOBILE', 'RAM', 'PLYMOUTH', 'GENESIS', 'OLDSMOBILE']\n",
    "          \n",
    "group3 = ['LEXUS', 'VOLVO', 'MERCEDES-BENZ', 'CADILLAC','LINCOLN', 'ACURA', 'BMW', 'SAAB', 'AUDI', 'INFINITI',\n",
    "          'LAND ROVER', 'MINI', 'JAGUAR', 'PORSCHE', 'HARLEY DAVIDSON']\n",
    "\n",
    "for i in group1:\n",
    "    df_household['sc_vehicle_manufacturer_name']=np.where(df_household['sc_vehicle_manufacturer_name']== i, 'STANDARD END VEHICLE', df_household['sc_vehicle_manufacturer_name'])\n",
    "for i in group2:\n",
    "    df_household['sc_vehicle_manufacturer_name']=np.where(df_household['sc_vehicle_manufacturer_name']== i, 'MIDDLE END VEHICLE', df_household['sc_vehicle_manufacturer_name'])\n",
    "for i in group3:\n",
    "    df_household['sc_vehicle_manufacturer_name']=np.where(df_household['sc_vehicle_manufacturer_name']== i, 'LUXURY VEHICLE', df_household['sc_vehicle_manufacturer_name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Occupation Code\n",
    "df_household.occupation_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.occupation_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Reviewing the graph above, there are 4 prodominat group\n",
    "* a) PROFESSIONAL\n",
    "* b) MANAGEMENT\n",
    "* c) RETIRED\n",
    "* d) OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1 = ['PROFESSIONAL - KNOWN', 'PROFESSIONAL - INFERRED', 'OFFICE ADMINISTRATION - KNOWN',\n",
    "         'SALES - KNOWN', 'OFFICE ADMINISTRATION - INFERRED', 'TECHNICAL - INFERRED',\n",
    "         'SALES - INFERRED', 'TECHNICAL - KNOWN']\n",
    "group2 = ['MANAGEMENT - KNOWN', 'BLUE COLLAR - KNOWN', 'MANAGEMENT - INFERRED', 'BLUE COLLAR - INFERRED']\n",
    "group3 = ['RETIRED - INFERRED', 'RETIRED - KNOWN']\n",
    "group4 = ['OTHER - INFERRED', 'OTHER - KNOWN']\n",
    "\n",
    "for i in group1:\n",
    "    df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'PROFESSIONAL', df_household['occupation_group'])\n",
    "for i in group2:\n",
    "    df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'MANAGEMENT', df_household['occupation_group'])\n",
    "for i in group3:\n",
    "    df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'RETIRED', df_household['occupation_group'])\n",
    "for i in group4:\n",
    "    df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'OTHER', df_household['occupation_group'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph occupant type\n",
    "df_household.occupant_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph member status\n",
    "df_household.member_status.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph deposit\n",
    "df_household.aaa_deposit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph Financial service\n",
    "df_household.aaa_financial_service.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph home_equity\n",
    "df_household.aaa_home_equity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph mortgage\n",
    "df_household.aaa_mortgage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph credit card\n",
    "df_household.aaa_credit_card.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph auto insurance\n",
    "df_household.aaa_auto_insurance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph id theft service\n",
    "df_household.aaa_id_theft.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to predict Roadside Service Usage by household key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Observation (Need update)\n",
    "\n",
    "Based on the previous anlysis, these are the most important feature:\n",
    "* income\n",
    "* tenure\n",
    "* credit_ranges\n",
    "* education\n",
    "* generation\n",
    "* race\n",
    "* home_owner\n",
    "* cancel_reason\n",
    "* mosaic_global_household\n",
    "* plus_indicator_description\n",
    "* Gender\n",
    "* kcl_b_ind_mosaicsgrouping\n",
    "* sc_vehicle_manufacturer_name\n",
    "* occupation_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re initiate all features and data\n",
    "def reiniciar(df_household):\n",
    "    df_household = read_data('household_view.csv')\n",
    "    \n",
    "    # Regroup Group Income\n",
    "    group1 = ['UNDER 10K','10-19,999' , '20-29,999', '30-39,999']\n",
    "    group2 = ['40-49,999', '50-59,999', '60-69,999', '70-79,999', '80-89,999', '90-99,999']\n",
    "    group3 = ['100-149,999', '150 - 174,999', '175 - 199,999', '200 - 249,999', '250K+' ]\n",
    "    for i in group1:\n",
    "        df_household['income']=np.where(df_household['income']== i, 'UNDER 39,999', df_household['income'])\n",
    "    for i in group2:\n",
    "        df_household['income']=np.where(df_household['income']== i, '40 - 99,999', df_household['income'])\n",
    "    for i in group3:\n",
    "        df_household['income']=np.where(df_household['income']== i, '100k+', df_household['income'])\n",
    "    \n",
    "    # Regroup tenures\n",
    "    group1 = ['BETWEEN 6 & 10 YEARS', 'BETWEEN 11 & 15 YEARS', \n",
    "          'BETWEEN 16 & 20 YEARS', 'BETWEEN 1 & 5 YEARS', '< 1 YEAR']\n",
    "    group2 = ['BETWEEN 31 & 40 YEARS', 'BETWEEN 21 & 30 YEARS']\n",
    "    group3 = ['+40 YEARS']\n",
    "    for i in group1:\n",
    "        df_household['tenure']=np.where(df_household['tenure']== i, 'UNDER 20 YEARS', df_household['tenure'])\n",
    "    for i in group2:\n",
    "        df_household['tenure']=np.where(df_household['tenure']== i, 'BETWEEN 21 & 40 YEARS', df_household['tenure'])\n",
    "    for i in group3:\n",
    "        df_household['tenure']=np.where(df_household['tenure']== i, '+40 YEARS', df_household['tenure'])\n",
    "    \n",
    "    # Regroup credit ranges\n",
    "    group1 = ['499 & LESS', '500-549','550-599','600-649', '650-699']\n",
    "    group2 = ['700-749', '750-799']\n",
    "    group3 = ['800+']\n",
    "    for i in group1:\n",
    "        df_household['credit_ranges']=np.where(df_household['credit_ranges']== i, 'BELOW 700', df_household['credit_ranges'])\n",
    "    for i in group2:\n",
    "        df_household['credit_ranges']=np.where(df_household['credit_ranges']== i, 'BETWEEN 700 & 799', df_household['credit_ranges'])\n",
    "    for i in group3:\n",
    "        df_household['credit_ranges']=np.where(df_household['credit_ranges']== i, '800+', df_household['credit_ranges'])\n",
    "    \n",
    "    # Regroup Education\n",
    "    df_household['education']=np.where(df_household['education'] =='GRADUATED SCHOOL', \n",
    "                                   'COMPLETED COLLEGE', df_household['education']\n",
    "                                  )\n",
    "    # Regroup Race\n",
    "    group1 = ['CAUCASION / WHITE - EUROPEAN', 'CAUCASION / WHITE - ENGLISH', 'CAUCASION / WHITE - WHITE NON-AMERICAN',\n",
    "          'CAUCASION / WHITE - UNKNOWN', 'CAUCASION / WHITE - EASTERN EUROPEAN', 'CAUCASION / WHITE - JEWISH',\n",
    "          'CAUCASION / WHITE - GREEK', 'CAUCASION / WHITE - DUTCH'\n",
    "         ]\n",
    "    group2 = ['HISPANIC - HISPANIC ORIGIN', 'HISPANIC - PORTUGUESE ORIGIN']\n",
    "    group3 = ['ASIAN - CHINESE', 'ASIAN - UNKNOWN', 'ASIAN - JAPANESE', 'ASIAN - INDIAN', 'ASIAN - ORIENTAL']\n",
    "    group4 = ['MIDDLE EASTERN - ARAB', 'MIDDLE EASTERN - EGYPTIAN']\n",
    "    group5 = ['AFRICAN AMERICAN - AFRICAN ORIGIN', 'AMERICAN INDIAN']\n",
    "\n",
    "    for i in group1:\n",
    "        df_household['race']=np.where(df_household['race']== i, 'CAUCASION / WHITE', df_household['race'])\n",
    "    for i in group2:\n",
    "        df_household['race']=np.where(df_household['race']== i, 'HISPANIC', df_household['race'])\n",
    "    for i in group3:\n",
    "        df_household['race']=np.where(df_household['race']== i, 'ASIAN', df_household['race'])\n",
    "    for i in group4:\n",
    "        df_household['race']=np.where(df_household['race']== i, 'ARABS', df_household['race'])\n",
    "    for i in group5:\n",
    "        df_household['race']=np.where(df_household['race']== i, 'AMERICAN AFRICANS / INDIANS', df_household['race'])\n",
    "\n",
    "    # Regroup Home Owner\n",
    "    df_household['home_owner']=np.where(df_household['home_owner']== 'PROBABLE RENTER', 'RENTER', df_household['home_owner'])\n",
    "    df_household['home_owner']=np.where(df_household['home_owner']== 'PROBABLE HOME OWNER', 'HOME OWNER', df_household['home_owner'])\n",
    "    \n",
    "    # Regroup Cancel reason\n",
    "    group1 = ['NON-PAYMENT', 'ERS DECLINE CC', 'BAD ADD SET BY PO ACS', '*BAD CHECKS*', \n",
    "         ]\n",
    "    group2 = ['DECEASED', 'NO LONGER DRIVING', 'MEMBERS REQUEST', 'CANCELLED DO NOT RENEW', 'TRANSFER MEMBER',\n",
    "              'DO NOT RENEW', 'MOVED OUT OF TERR', 'TRANSFER MEMBER SUSPEND', 'CANCELLED MBRS REQUEST',\n",
    "              'HEALTH ISSUE', 'NONCOMPLIANT AGE', 'PO RETURN'\n",
    "             ]\n",
    "    group3 = ['DATA PROBLEM','DUP MEMBERSHIP','ERS CNCL MSHIP PROGRAM','MANAGEMENT REQUEST', 'CANCEL PROCESS - FUTURE CANCEL DNR']\n",
    "\n",
    "    for i in group1:\n",
    "        df_household['cancel_reason']=np.where(df_household['cancel_reason']== i, 'PAYMENT ISSUES', df_household['cancel_reason'])\n",
    "    for i in group2:\n",
    "        df_household['cancel_reason']=np.where(df_household['cancel_reason']== i, 'MEMBER ISSUES', df_household['cancel_reason'])\n",
    "    for i in group3:\n",
    "        df_household['cancel_reason']=np.where(df_household['cancel_reason']== i, 'DATA/AAA ISSUES', df_household['cancel_reason'])\n",
    "\n",
    "    # Regroup Mosaic Global household\n",
    "    df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'METROPOLITAN STRUGGLERS', \n",
    "                                                 'ROUTINE SERVICE WORKERS', df_household['mosaic_global_household']\n",
    "                                                )\n",
    "\n",
    "    df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'SOPHISTICATED SINGLES', \n",
    "                                                     'BOURGEOIS PROSPERITY', df_household['mosaic_global_household']\n",
    "                                                    )\n",
    "\n",
    "    df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'RURAL INHERITANCE', \n",
    "                                                     'CAREER AND FAMILY', df_household['mosaic_global_household']\n",
    "                                                    )\n",
    "    df_household['mosaic_global_household']=np.where(df_household['mosaic_global_household']== 'POST INDUSTRIAL SURVIVORS', \n",
    "                                                     'ROUTINE SERVICE WORKERS', df_household['mosaic_global_household']\n",
    "                                                    )\n",
    "    # Regroup Experian Group\n",
    "    group1 = ['AUTUMN YEARS', 'GOLDEN YEAR GUARDIANS']\n",
    "    group2 = ['BOOMING WITH CONFIDENCE', 'THRIVING BOOMERS', 'BLUE SKY BOOMERS']\n",
    "    group3 = ['POWER ELITE', 'SURBURBAN STYLE', 'FLOURISHING FAMILIES']\n",
    "    group4 = ['FAMILY UNION', 'MIDDLE CLASS MELTING POT', 'PROMISING FAMILIES', 'ECONOMIC CHALLENGES', 'FAMILIES IN MOTION',\n",
    "              'PASTORAL PRIDE']\n",
    "    group5 = ['SINGLES AND STARTERS', 'SIGNIFICANT SINGLES', 'YOUNG CITY SOLOS', 'CULTURAL CONNECTIONS',\n",
    "             'ASPIRATIONAL FUSION']\n",
    "    for i in group1:\n",
    "        df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'GOLDEN YEAR', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "    for i in group2:\n",
    "        df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'BOOMERS', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "    for i in group3:\n",
    "        df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'POWER ELITE', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "    for i in group4:\n",
    "        df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'MIDDLE CLASS MELTING POT', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "    for i in group5:\n",
    "        df_household['kcl_b_ind_mosaicsgrouping']=np.where(df_household['kcl_b_ind_mosaicsgrouping']== i, 'SINGLES AND STARTERS', df_household['kcl_b_ind_mosaicsgrouping'])\n",
    "    \n",
    "    # Regroup Vehicle Manufaturer\n",
    "    group1 = [ 'TOYOTA', 'FORD', 'HONDA', 'CHEVROLET', 'HYUNDAI', 'NISSAN', 'BUICK', 'SUBARU', 'SUZUKI',\n",
    "         'DODGE', 'CHRYSLER', 'VOLKSWAGEN','KIA', 'MAZDA', 'SATURN', 'PONTIAC', 'SCION', 'MITSUBISHI',\n",
    "          'GEO', 'CRYSLER', 'MG', 'FIAT', 'BICYCLE', 'ISUZU']\n",
    "    group2 = ['JEEP', 'MERCURY','GMC', 'OLSMOBILE', 'RAM', 'PLYMOUTH', 'GENESIS', 'OLDSMOBILE']\n",
    "\n",
    "    group3 = ['LEXUS', 'VOLVO', 'MERCEDES-BENZ', 'CADILLAC','LINCOLN', 'ACURA', 'BMW', 'SAAB', 'AUDI', 'INFINITI',\n",
    "              'LAND ROVER', 'MINI', 'JAGUAR', 'PORSCHE', 'HARLEY DAVIDSON']\n",
    "\n",
    "    for i in group1:\n",
    "        df_household['sc_vehicle_manufacturer_name']=np.where(df_household['sc_vehicle_manufacturer_name']== i, 'STANDARD END VEHICLE', df_household['sc_vehicle_manufacturer_name'])\n",
    "    for i in group2:\n",
    "        df_household['sc_vehicle_manufacturer_name']=np.where(df_household['sc_vehicle_manufacturer_name']== i, 'MIDDLE END VEHICLE', df_household['sc_vehicle_manufacturer_name'])\n",
    "    for i in group3:\n",
    "        df_household['sc_vehicle_manufacturer_name']=np.where(df_household['sc_vehicle_manufacturer_name']== i, 'LUXURY VEHICLE', df_household['sc_vehicle_manufacturer_name'])\n",
    "\n",
    "    # Regroup Occupation Group\n",
    "    group1 = ['PROFESSIONAL - KNOWN', 'PROFESSIONAL - INFERRED', 'OFFICE ADMINISTRATION - KNOWN',\n",
    "         'SALES - KNOWN', 'OFFICE ADMINISTRATION - INFERRED', 'TECHNICAL - INFERRED',\n",
    "         'SALES - INFERRED', 'TECHNICAL - KNOWN']\n",
    "    group2 = ['MANAGEMENT - KNOWN', 'BLUE COLLAR - KNOWN', 'MANAGEMENT - INFERRED', 'BLUE COLLAR - INFERRED']\n",
    "    group3 = ['RETIRED - INFERRED', 'RETIRED - KNOWN']\n",
    "    group4 = ['OTHER - INFERRED', 'OTHER - KNOWN']\n",
    "\n",
    "    for i in group1:\n",
    "        df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'PROFESSIONAL', df_household['occupation_group'])\n",
    "    for i in group2:\n",
    "        df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'MANAGEMENT', df_household['occupation_group'])\n",
    "    for i in group3:\n",
    "        df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'RETIRED', df_household['occupation_group'])\n",
    "    for i in group4:\n",
    "        df_household['occupation_group']=np.where(df_household['occupation_group']== i, 'OTHER', df_household['occupation_group'])\n",
    "\n",
    "    # Regroup Total members inside a household\n",
    "    df_household['no_members'] = ' '\n",
    "    df_household['no_members']=np.where(df_household['total_members_in_household']== 1, '1 - MEMBER', \n",
    "                                        df_household['no_members'])\n",
    "    df_household['no_members']=np.where(df_household['total_members_in_household']== 2, '2 - MEMBER', \n",
    "                                        df_household['no_members'])\n",
    "    df_household['no_members']=np.where(df_household['total_members_in_household']== 3, '3 - MEMBER', \n",
    "                                        df_household['no_members'])\n",
    "    df_household['no_members']=np.where(df_household['total_members_in_household']== 4, '4 - MEMBER', \n",
    "                                        df_household['no_members'])\n",
    "\n",
    "    df_household['no_members']=np.where(df_household['total_members_in_household']> 4, '5 - MEMBER', \n",
    "                                        df_household['no_members'])\n",
    "    group5 = np.arange(5,20,1)\n",
    "    for i in group5:\n",
    "        df_household['no_members']=np.where(df_household['total_members_in_household']== i, '+5 - MEMBER', \n",
    "                                        df_household['no_members'])\n",
    "    \n",
    "    # Create roadside usage\n",
    "    df_household['use_road_side'] = df_household['total_calls'].apply(lambda x:road_side_usage(x))\n",
    "    df_household.consumer_score = np.where(df_household.consumer_score!= np.nan, 1/df_household.consumer_score, \n",
    "                                        df_household.consumer_score)\n",
    "    # Remove members with status CANCELLED\n",
    "    df_household = df_household[df_household.member_status!='CANCELLED']\n",
    "    return df_household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.groupby(by=var_to_predict[0])['total_calls'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.groupby(by=var_to_predict[0])['total_member_cost'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.groupby(by=var_to_predict[0])['basic_cost'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.groupby(by=var_to_predict[0])['plus_cost'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_household.groupby(by=var_to_predict[0])['premier_cost'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload data to conduct another experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_household.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Numeric Columns\n",
    "col_num =['aaa_member_tenure_years','aaa_mean_age','length_of_residence','aaa_mortgage','aaa_credit_card','aaa_deposit',\n",
    "          'aaa_home_equity', 'aaa_financial_service','aaa_auto_insurance','aaa_id_theft','aaa_motorcycle_indicator',\n",
    "          'aaa_travel','aaa_mean_child','aaa_no_race','aaa_no_home_owner','aaa_no_education','aaa_no_income',\n",
    "          'aaa_no_dwelling_type','aaa_no_credit_ranges','aaa_no_gender','aaa_no_language','aaa_no_reason_joined',\n",
    "          'aaa_no_mosaic_household','aaa_no_mosaic_global_household', 'aaa_no_kcl_b_ind_mosaicsgrouping',\n",
    "          'aaa_no_occupation_code','aaa_no_occupation_group','aaa_no_occupant_type','aaa_no_plus_indicator_description',\n",
    "          'aaa_no_generation','total_member_cost_1','total_member_cost_2','total_member_cost_3','basic_cost','plus_cost',\n",
    "          'premier_cost','total_calls','total_member_cost','mean_total_member_cost','total_tow_miles','mean_tow_miles',\n",
    "          'total_members_in_household','total_calls_veh','total_cost_veh','total_member_cost_veh','mean_total_calls_veh',\n",
    "          'mean_total_cost_veh','mean_total_member_cost_veh','consumer_score','use_road_side'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all numeric features from df_household\n",
    "# Get only numeric columns\n",
    "# col_num = df_household.select_dtypes(include=np.number).columns.tolist()\n",
    "# Removed: basic_costs, plus_cost, total_member_cost, total_cost_veh - All highly correlated so decided to remove them\n",
    "col_num =['length_of_residence','aaa_mean_child','aaa_motorcycle_indicator',\n",
    "          'mean_premier_cost','total_calls','mean_tow_miles',\n",
    "          'total_members_in_household', 'mean_total_cost_veh',\n",
    "          'consumer_score','use_road_side', 'mean_total_calls_veh',\n",
    "          'mean_plus_cost', 'mean_total_member_cost'\n",
    "         ]\n",
    "df_corr = pd.DataFrame(df_household, columns=col_num)\n",
    "df_corr = df_corr.corr()\n",
    "plt.figure(figsize=(20,16))\n",
    "sns.heatmap(df_corr, annot=True, fmt=\".3f\", linewidths=0.1, xticklabels=True, \n",
    "                cbar_kws={'label': var_to_predict_title}, yticklabels=True)\n",
    "bottom, top = plt.ylim()\n",
    "plt.ylim(bottom + 0.7, top - 0.7)\n",
    "plt.xticks(rotation=90, \n",
    "           horizontalalignment='right',\n",
    "           fontweight='light',\n",
    "           fontsize='large')\n",
    "plt.savefig(var_to_predict_save+'_var_corr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data to conduct another experiment\n",
    "df_household = reiniciar(df_household)\n",
    "cols_house_dummmies = ['aaa_financial_service','plus_indicator_description', 'aaa_id_theft', 'generation',\n",
    "                       'occupation_group','sc_vehicle_manufacturer_name', 'tenure', 'aaa_auto_insurance',\n",
    "                       'no_members', 'gender', 'kcl_b_ind_mosaicsgrouping','education','home_owner'\n",
    "                      ]\n",
    "# Get features from previous heatmap\n",
    "feature_cols = col_num.copy()\n",
    "\n",
    "# Build Dummies Variables\n",
    "df_dummies = pd.get_dummies(df_household, columns=cols_house_dummmies)\n",
    "for i in cols_house_dummmies:\n",
    "    cols = [j for j in df_dummies.columns if j.startswith(i)]\n",
    "    feature_cols += cols\n",
    "\n",
    "# construct Dataframe for prediction\n",
    "feature_cols += var_to_predict\n",
    "df_prediction = pd.DataFrame(df_dummies, columns=feature_cols)\n",
    "\n",
    "# Stripping out spaces from ends of names, and replacing internal spaces with \"_\"\n",
    "df_prediction.columns = [col.strip().replace(' ', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('&', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('+', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace(',', '').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('(', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace(')', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('<', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('.', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('/', '_').lower() for col in df_prediction.columns]\n",
    "df_prediction.columns = [col.strip().replace('-', '_').lower() for col in df_prediction.columns]\n",
    "\n",
    "# Fix Nans\n",
    "# Number of child\n",
    "df_prediction.aaa_mean_child.fillna(value=0, inplace=True)\n",
    "df_prediction.consumer_score.fillna(value=0, inplace=True)\n",
    "# Drop Unknown Columns \n",
    "cols_to_drop = [j for j in df_prediction.columns if j.endswith('unknown')]\n",
    "df_prediction.drop(labels=cols_to_drop,axis=1, inplace=True)\n",
    "\n",
    "# Save columns name for future use in modelling to split X-Input and y-Output\n",
    "X_cols = df_prediction.columns[:-1]\n",
    "y_cols = var_to_predict.copy()\n",
    "\n",
    "df_prediction.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Correlation of AAA Costs and other numeric features\n",
    "# Heat Map \n",
    "df_corr = df_prediction.corr()\n",
    "df_corr.sort_values(by=var_to_predict[0], ascending=False, inplace=True)\n",
    "df_corr = df_corr[df_corr.index.values!=var_to_predict[0]]\n",
    "sns.heatmap(df_corr[[var_to_predict[0]]], annot=True, fmt=\".3f\", linewidths=0.1, xticklabels=True, \n",
    "                cbar_kws={'label': var_to_predict_title}, yticklabels=True)\n",
    "bottom, top = plt.ylim()\n",
    "plt.ylim(bottom + 0.7, top - 0.7)\n",
    "plt.savefig(var_to_predict_save+'_var_corr_tot_cost')\n",
    "plt.show()\n",
    "#df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot \n",
    "sns.pairplot(df_prediction[['mean_total_cost', 'mean_tow_miles', 'total_members_in_household', \n",
    "                            'consumer_score','use_road_side', 'total_calls']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset figure and display settings\n",
    "%matplotlib inline\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "rcParams.update({'figure.autolayout': True, 'figure.figsize':(12,8),'axes.titlesize':14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Training and Test data for up Sampling the minority class\n",
    "To use the same split in all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Search and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "from sklearn import base\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "\n",
    "estimators = all_estimators()\n",
    "for name, class_ in estimators:\n",
    "    if issubclass(class_, base.RegressorMixin):\n",
    "        print(name)\n",
    "\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "np.random.seed(42)\n",
    "scoring = 'neg_mean_squared_error'\n",
    "test_size = 0.30\n",
    "\n",
    "# Create input and output dataframes\n",
    "\n",
    "X_df = pd.DataFrame(df_prediction,columns=X_cols)\n",
    "y_df = pd.DataFrame(df_prediction,columns=y_cols)\n",
    "\n",
    "# Transform / Normalize the data\n",
    "scaler_x = PowerTransformer()\n",
    "scaled_df_x = scaler_x.fit_transform(X_df)\n",
    "scaled_df_x = pd.DataFrame(scaled_df_x, columns=X_df.columns)\n",
    "\n",
    "scaler_y = PowerTransformer()\n",
    "scaled_df_y = scaler_y.fit_transform(y_df)\n",
    "scaled_df_y = pd.DataFrame(scaled_df_y, columns=y_df.columns)\n",
    "\n",
    "\n",
    "# Split Train and test\n",
    "trainX, testX, trainy, testy = train_test_split(scaled_df_x, scaled_df_y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Correlation of AAA Costs and other numeric features\n",
    "# Heat Map \n",
    "df_corr = scaled_df_x.copy()\n",
    "df_corr[var_to_predict] = scaled_df_y[var_to_predict]\n",
    "df_corr = df_corr.corr()\n",
    "df_corr.sort_values(by=var_to_predict[0], ascending=False, inplace=True)\n",
    "df_corr = df_corr[df_corr.index.values!=var_to_predict[0]]\n",
    "sns.heatmap(df_corr[[var_to_predict[0]]], annot=True, fmt=\".3f\", linewidths=0.1, xticklabels=True, \n",
    "                cbar_kws={'label': var_to_predict_title+' - Transformed'}, yticklabels=True)\n",
    "bottom, top = plt.ylim()\n",
    "plt.ylim(bottom + 0.7, top - 0.7)\n",
    "plt.savefig(var_to_predict_save+'_var_corr_tot_cost_transf')\n",
    "plt.show()\n",
    "#df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms Search\n",
    "# Spot Check Algorithms\n",
    "\n",
    "# Linear Regression Methods\n",
    "print(var_to_predict_title+' - Algorithm Comparison - Regression')\n",
    "models = []\n",
    "#models.append(('OLS', LinearRegression(normalize=False)))\n",
    "models.append(('BR', BayesianRidge(normalize=False)))\n",
    "models.append(('DTR', DecisionTreeRegressor()))\n",
    "models.append(('EN', ElasticNet(normalize=False)))\n",
    "models.append(('ENCV', ElasticNetCV(normalize=False)))\n",
    "models.append(('LASSO', Lasso(normalize=False)))\n",
    "models.append(('LASCV', LassoCV(normalize=False)))\n",
    "models.append(('LSVR', LinearSVR()))\n",
    "models.append(('SGDR', SGDRegressor()))\n",
    "models.append(('KNNR', KNeighborsRegressor()))\n",
    "models.append(('RIDGE', Ridge(normalize=False)))\n",
    "models.append(('RIDCV', RidgeCV(normalize=False)))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(model, trainX, trainy, cv=kfold, scoring=scoring)\n",
    "    # Calculate RMSE\n",
    "    results.append(np.sqrt(np.abs(cv_results)))\n",
    "    names.append(name)                          \n",
    "    msg = \"%s: RMSE : %.3f - Std. Dev.: (%.3f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Ensembles Methods\n",
    "ensembles = []\n",
    "ensembles.append(('ABR', AdaBoostRegressor()))\n",
    "ensembles.append(('GBR', GradientBoostingRegressor()))\n",
    "ensembles.append(('RFR', RandomForestRegressor(n_estimators=10)))\n",
    "ensembles.append(('ET', ExtraTreesRegressor(n_estimators=10)))\n",
    "ensembles.append(('XGB', XGBRegressor(silent=True)))\n",
    "ensembles.append(('BGR', BaggingRegressor()))\n",
    "\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(model, trainX, trainy, cv=kfold, scoring=scoring)\n",
    "    results.append(np.sqrt(np.abs(cv_results)))\n",
    "    names.append(name)\n",
    "    msg = \"%s: RMSE : %.3f - Std. Dev.: (%.3f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle(var_to_predict_title+' - Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results, meanline=True, showmeans=True)\n",
    "ax.set_xticklabels(names)\n",
    "plt.savefig(var_to_predict_save+'_algorithms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "R^2 (coefficient of determination) regression score function.\n",
    "\n",
    "    * Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that\n",
    "    always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "\n",
    "mean_squared_error\n",
    "    * A non-negative floating point value (the best value is 0.0)\n",
    "    The MSE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero are better.\n",
    "\n",
    "Based on the graph above (RMSE):\n",
    "    * 1) GBR: RMSE : -0.246 - Std. Dev.: (0.047)\n",
    "    * 2) XGB: RMSE : -0.247 - Std. Dev.: (0.048)\n",
    "    * 3) BGR: RMSE : -0.283 - Std. Dev.: (0.060)\n",
    "    * 4) RFR: RMSE : -0.283 - Std. Dev.: (0.060)\n",
    "    * 5) ET : RMSE : -0.326 - Std. Dev.: (0.067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search with Up Sampling\n",
    "# Logistic regression\n",
    "np.random.seed(42)\n",
    "xgbr_param_grid = {'colsample_bytree': [0.1, 0.3, 0.5, 0.7, 0.9],'n_estimators': [50, 750, 250, 500, 1000],\n",
    "                  'max_depth': [5,6,7,8], 'learning_rate': [0.1, 0.01, 0.05, 1],\n",
    "                 }\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "xgbr = XGBRegressor(silent=True)\n",
    "# Perform grid search: grid_mse\n",
    "xgbr_grid = GridSearchCV(estimator=xgbr, param_grid=xgbr_param_grid,\n",
    "                       scoring=scoring, cv=kfold, verbose=1, n_jobs=20)\n",
    "xgbr_grid.fit(trainX, trainy)\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", xgbr_grid.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(xgbr_grid.best_score_)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "These are the best parameters:\n",
    "\n",
    "First run identifying C\n",
    "    * Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000}\n",
    "    * Lowest RMSE found:  0.0628491240388626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling based on the outcome of the grid serach of the random forest\n",
    "xgbr = XGBRegressor(silent=True, colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=500)\n",
    "\n",
    "xgbr.fit(trainX, trainy)\n",
    "\n",
    "# Predict on test set\n",
    "yhat_train = xgbr.predict(trainX)\n",
    "\n",
    "# Predict on test set\n",
    "yhat = xgbr.predict(testX)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "scores_xgbr = cross_val_score(xgbr, testX, testy, cv=num_folds, scoring=scoring)\n",
    "\n",
    "print('XGBoost Classifer Train r2 score - Transformed : %.5f' % r2_score(trainy, yhat_train))\n",
    "print('XGBoost Classifer Test r2 score - Transformed  : %.5f' % r2_score(testy, yhat))\n",
    "yhat_train_mse1 = mean_squared_error(trainy, yhat_train)\n",
    "yhat_test_mse1 = mean_squared_error(testy, yhat)\n",
    "yhat_train_rmse1 = np.sqrt(yhat_train_mse1)\n",
    "yhat_test_rmse1 = np.sqrt(yhat_test_mse1)\n",
    "print('XGBoost Classifer  Train MSE - Transformed    : %.5f' % yhat_train_mse1)\n",
    "print('XGBoost Classifer  Test MSE - Transformed     : %.5f' % yhat_test_mse1)\n",
    "print('XGBoost Classifer  Train RMSE - Transformed   : %.5f' % yhat_train_rmse1)\n",
    "print('XGBoost Classifer  Test RMSE - Transformed    : %.5f' % yhat_test_rmse1)\n",
    "\n",
    "# Plot Cross Validation RMSE Scores\n",
    "fig = px.line(np.sqrt(np.abs(scores_xgbr)), y=np.sqrt(np.abs(scores_xgbr)), labels={'y':'RMSE'}, title=var_to_predict_title+ ' - RMSE Cross Validation Scores - Transformed')\n",
    "fig.write_image(var_to_predict_save+'_xgbr_crossv.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untransformed series\n",
    "un_trainy = trainy.copy()\n",
    "un_trainy[var_to_predict] = scaler_y.inverse_transform(un_trainy) \n",
    "\n",
    "un_testy = testy.copy()\n",
    "un_testy[var_to_predict] = scaler_y.inverse_transform(un_testy) \n",
    "\n",
    "un_yhat_train = pd.DataFrame(data=scaler_y.inverse_transform(yhat_train.reshape(-1, 1)), \n",
    "                                  index=trainy.index, columns=var_to_predict) \n",
    "\n",
    "un_yhat = pd.DataFrame(data=scaler_y.inverse_transform(yhat.reshape(-1, 1)), \n",
    "                                  index=testy.index, columns=var_to_predict) \n",
    "\n",
    "# Build Training Dataframe\n",
    "df_train = un_trainy.copy()\n",
    "df_train['FCST Total Cost Train'] = un_yhat_train[var_to_predict]\n",
    "df_train['FCST Error Train'] = 0\n",
    "df_train['FCST Error Train'] = df_train[var_to_predict] - df_train['FCST Total Cost Train']\n",
    "\n",
    "# Build test dataframe\n",
    "df_test = un_testy.copy()\n",
    "df_test['FCST Total Cost Test'] = un_yhat[var_to_predict]\n",
    "df_test['FCST Error Test'] = 0\n",
    "df_test['FCST Error Test'] = df_test[var_to_predict] - df_test['FCST Total Cost Test']\n",
    "\n",
    "print('XGBoost Regressor Train r2 score : %.5f' % r2_score(un_trainy, un_yhat_train))\n",
    "print('XGBoost Regressor Test  r2 score : %.5f' % r2_score(un_testy, un_yhat))\n",
    "un_yhat_train_mse1 = mean_squared_error(un_trainy, un_yhat_train)\n",
    "un_yhat_test_mse1 = mean_squared_error(un_testy, un_yhat)\n",
    "un_yhat_train_rmse1 = np.sqrt(un_yhat_train_mse1)\n",
    "un_yhat_test_rmse1 = np.sqrt(un_yhat_test_mse1)\n",
    "print('XGBoost Regressor  Train MSE             : %.5f' % un_yhat_train_mse1)\n",
    "print('XGBoost Regressor  Test MSE              : %.5f' % un_yhat_test_mse1)\n",
    "print('XGBoost Regressor  Train RMSE            : %.5f' % un_yhat_train_rmse1)\n",
    "print('XGBoost Regressor  Test RMSE             : %.5f' % un_yhat_test_rmse1)\n",
    "print('XGBoost Regressor  Train Mean FCST Error : %.5f' % df_train['FCST Error Train'].mean())\n",
    "print('XGBoost Regressor  Test Mean FCST Error  : %.5f' % df_test['FCST Error Test'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_test['FCST Error Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "sns.scatterplot(data=df_test[var_to_predict], color=\"b\", legend='full', ax=ax1)\n",
    "sns.scatterplot(data=df_test['FCST Total Cost Test'], color=\"r\", legend='full', ax=ax1)\n",
    "sns.scatterplot(data=df_test['FCST Error Test'], color=\"g\", legend='full', ax=ax2)\n",
    "plt.title(var_to_predict_title+ ' - (Real/Estimate/Error)',fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Graph Train prediction\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : True})\n",
    "g = sns.lmplot(x=var_to_predict[0], y='FCST Total Cost Train', data=df_train, height=10, aspect=1.2)\n",
    "g.set_axis_labels(\"Total Costs - Actual\", \"Forecast Total Costs - Train\") \n",
    "plt.savefig(var_to_predict_save+'_fcst_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Test prediction\n",
    "g = sns.lmplot(x=var_to_predict[0], y='FCST Total Cost Test', data=df_test, height=10, aspect=1.2)\n",
    "g.set_axis_labels(\"Total Costs - Actual\", \"Forecast Total Costs - Test\") \n",
    "plt.savefig(var_to_predict_save+'_fcst_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(un_yhat_train, y=var_to_predict[0], labels={var_to_predict[0]:var_to_predict_title}, title=var_to_predict_title+ ' - RMSE Cross Validation Scores - Transformed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use Logistic Regression algorithms to classify which customers are going to use Deposit Service or not (0 = No, 1 = Yes)\n",
    "# Using Oversample Dataframe with transformation MinMax Scaler\n",
    "\n",
    "np.random.seed(42)\n",
    "# Define model \n",
    "lr = LogisticRegression(penalty='l2',multi_class=\"auto\", solver=\"liblinear\", C=92.01)\n",
    "\n",
    "# Fit model\n",
    "lr.fit(trainX, trainy)\n",
    "\n",
    "# Predict on test set\n",
    "yhat = lr.predict(testX)\n",
    "\n",
    "# Generate a no skill prediction for ROC Curve\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# Predict probabilities\n",
    "lr_probs = lr.predict_proba(testX)\n",
    "\n",
    "# Keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "\n",
    "# Calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "lr_scores = cross_val_score(lr, testX, testy, cv=num_folds)\n",
    "\n",
    "# Evaluate predictions\n",
    "print('Logistic Regression - Accuracy training          : %.3f' % accuracy_score(lr.predict(trainX), trainy))\n",
    "print('Logistic Regression - Accuracy test              : %.3f' % accuracy_score(testy, yhat))\n",
    "print('Logistic Regression - Precision                  : %.3f' % precision_score(testy, yhat))\n",
    "print('Logistic Regression - Recall                     : %.3f' % recall_score(testy, yhat))\n",
    "print('Logistic Regression - F-measure                  : %.3f' % f1_score(testy, yhat))\n",
    "print('Logistic Regression - Log-Loss                   : %.3f' % log_loss(testy, yhat))\n",
    "print('Logistic Regression - Cross Entropy Loss         : %.3f' % cross_entropy(yhat, testy[var_to_predict[0]]))\n",
    "print('Logistic Regression - No Skill - ROC AUC         : %.3f' % (ns_auc))\n",
    "print('Logistic Regression - ROC AUC                    : %.3f' % (lr_auc))\n",
    "print('Logistic Regression - Cross Validation Accuracy  : %0.3f (+/- %0.3f)' % (lr_scores.mean(), lr_scores.std() * 2))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill - '+var_to_predict_title)\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic - '+var_to_predict_title)\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# Show title\n",
    "plt.title('Logistic Regression - ROC Curve - '+var_to_predict_title, loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "# show the plot\n",
    "plt.savefig(var_to_predict_save+'_lr_roc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train / Cross Validation Score Plot\n",
    "skplt.estimators.plot_learning_curve(lr, trainX, trainy, cv=num_folds, title=var_to_predict_title + ' - Log. Regression - Train / Cross Validation Score Plot')\n",
    "plt.savefig(var_to_predict_save+'_lr_txcval')\n",
    "plt.show()\n",
    "\n",
    "# Cummulative Gain Plot\n",
    "skplt.metrics.plot_cumulative_gain(testy,lr.predict_proba(testX), title=var_to_predict_title + ' - Log. Regression -  Cummulative Gains Curve')\n",
    "plt.savefig(var_to_predict_save+'_lr_cgc')\n",
    "plt.show()\n",
    "\n",
    "# Lift Curve Plot\n",
    "skplt.metrics.plot_lift_curve(testy,lr.predict_proba(testX), title=var_to_predict_title + '  - Log. Regression - Lift Curve')\n",
    "plt.savefig(var_to_predict_save+'_lr_liftc')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve\n",
    "skplt.metrics.plot_precision_recall_curve(testy,lr.predict_proba(testX), title=var_to_predict_title + '  - Log. Regression - Precision - Recall Curve')\n",
    "plt.savefig(var_to_predict_save+'_lr_prerecc')\n",
    "plt.show()\n",
    "\n",
    "# KS statistics Plot\n",
    "skplt.metrics.plot_ks_statistic(testy, lr.predict_proba(testX), title=var_to_predict_title+'  - Log. Regression - KS Statistics Plot')\n",
    "plt.savefig(var_to_predict_save+'_lr_ksst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion Metrics\n",
    "np.set_printoptions(precision=2)\n",
    "i = 1\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix - \"+var_to_predict_title+\", without normalization - Logistic Regression\", None),\n",
    "                  (\"Normalized confusion matrix - \"+var_to_predict_title + \" - Logistic Regression\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(lr, testX, testy,\n",
    "                                 display_labels=testy,\n",
    "                                 values_format='.3f',\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "    if i==1: plt.savefig(var_to_predict_save+'_lr_cm')\n",
    "    else: plt.savefig(var_to_predict_save+'_lr_cm_1')\n",
    "    i +=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_df_lgr = pd.DataFrame({'feature':testX.columns, 'coefs': lr.coef_[0]})\n",
    "coef_df_lgr[\"abs_coeff\"] = coef_df_lgr['coefs'].abs()\n",
    "coef_df_lgr.sort_values(\"abs_coeff\",ascending=False,inplace=True)\n",
    "coef_df_lgr.nlargest(30,columns=[\"abs_coeff\"])\n",
    "counts_df_lgr = pd.DataFrame(coef_df_lgr.nlargest(30,columns=[\"abs_coeff\"]), index=None)\n",
    "#counts.reset_index(inplace=True)\n",
    "counts_df_lgr.rename(columns={\"feature\": \"Features (MinMax Transformer)\", 'coefs':'Coefficients', 'abs_coeff': \"ABS of Coeffs.\"}, inplace=True)\n",
    "fig = ff.create_table(counts_df_lgr, height_constant=30, index=False)\n",
    "fig.write_image(var_to_predict_save+'_lr_coef.png')\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bar graph of Coefficients\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.axhline(0, color=\"k\", clip_on=False)\n",
    "plt.ylabel(\"Coefs\",fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title('Features of Logistic Regression Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "sns.barplot(y=counts_df_lgr['Features (MinMax Transformer)'], x=counts_df_lgr['Coefficients'], orient='h')\n",
    "plt.xticks(\n",
    "    rotation=90, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='small')\n",
    "plt.savefig(var_to_predict_save+'_lr_coef_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model - Grid search with RandomizedSearch\n",
    "np.random.seed(42)\n",
    "num_folds = 10\n",
    "scoring='accuracy'\n",
    "weight_options = ['uniform', 'distance']\n",
    "algorithm_options = ['ball_tree', 'kd_tree', 'brute']\n",
    "neighbors_settings = list(range(1, 10))\n",
    "leaf_options = list(range(1, 50))\n",
    "param_grid = dict(n_neighbors=neighbors_settings, weights=weight_options, algorithm=algorithm_options, leaf_size=leaf_options)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "best_scores = []\n",
    "for _ in range(10):\n",
    "    rand = RandomizedSearchCV(knn, param_grid, cv=num_folds, scoring=scoring, n_iter=10)\n",
    "    rand.fit(trainX,trainy)\n",
    "    best_scores.append([rand.best_params_,rand.best_score_])\n",
    "\n",
    "for i in range(len(best_scores)):\n",
    "    print('The best parameters : ',best_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model - Grid search\n",
    "# The best parameters :\n",
    "    # The best parameters :  [{'weights': 'distance', 'n_neighbors': 2, 'leaf_size': 36, 'algorithm': 'kd_tree'}, 0.7809109009457441]\n",
    "np.random.seed(42)\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to 20\n",
    "neighbors_settings = range(1, 6)\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # build the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors,  weights='distance', leaf_size=36, algorithm='kd_tree')\n",
    "    knn.fit(trainX, trainy)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(knn.score(trainX, trainy))\n",
    "    # record test set accuracy\n",
    "    test_accuracy.append(knn.score(testX, testy))\n",
    "\n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"Training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"Test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "# Show title\n",
    "plt.title('KNN Neighbors Grid Search - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "plt.savefig(var_to_predict_save+'_knn_compare_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "The plot shows the training and test set accuracy on the y-axis against the setting of n_neighbors on the x-axis. Considering a single nearest neighbor, the prediction on the training set is perfect. But when more neighbors are considered, the training accuracy drops, indicating that using more than 2 neighbor leads to a model that is too complex.\n",
    "\n",
    "The best performance is somewhere around 2 neighbors. Still, it is good to keep the scale of the plot in mind. The worst performance is more than 82% accuracy, which might still be pretty good.\n",
    "\n",
    "The above plot suggests that we should choose n_neighbors= where the model gets its stability. \n",
    "Outcome of the Grid Search above \n",
    "* The best parameters : The best parameters :  The best parameters :  [{'weights': 'uniform', 'n_neighbors': 1, 'leaf_size': 22, 'algorithm': 'ball_tree'}, 0.7895104895104895]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "# The best parameters :  [{'weights': 'distance', 'n_neighbors': 1, 'leaf_size': 36, 'algorithm': 'kd_tree'}, 0.7809109009457441]\n",
    "knn = KNeighborsClassifier(n_neighbors=1) #, leaf_size=36,algorithm='kd_tree')\n",
    "knn.fit(trainX, trainy)\n",
    "\n",
    "\n",
    "print('Accuracy of K-NN classifier on training set: {:.3f}'.format(knn.score(trainX, trainy)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.3f}'.format(knn.score(testX, testy)))\n",
    "\n",
    "# Predict on test set\n",
    "yhat = knn.predict(testX)\n",
    "\n",
    "# Generate a no skill prediction for ROC Curve\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# Predict probabilities\n",
    "knn_probs = knn.predict_proba(testX)\n",
    "\n",
    "# Keep probabilities for the positive outcome only\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "# Calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "knn_auc = roc_auc_score(testy, knn_probs)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "knn_scores = cross_val_score(knn, testX, testy, cv=num_folds)\n",
    "\n",
    "# Evaluate predictions\n",
    "print('K-NN classifier - Precision                  : %.3f' % precision_score(testy, yhat))\n",
    "print('K-NN classifier - Recall                     : %.3f' % recall_score(testy, yhat))\n",
    "print('K-NN classifier - F-measure                  : %.3f' % f1_score(testy, yhat))\n",
    "print('K-NN classifier - Log-Loss                   : %.3f' % log_loss(testy, yhat))\n",
    "print('K-NN Classifier - Cross Entropy Loss         : %.3f' % cross_entropy(yhat, testy[var_to_predict[0]]))\n",
    "print('K-NN classifier - No Skill - ROC AUC         : %.3f' % (ns_auc))\n",
    "print('K-NN classifier - ROC AUC                    : %.3f' % (knn_auc))\n",
    "print(\"K-NN classifier - Cross Validation Accuracy  : %0.3f (+/- %0.3f)\" % (knn_scores.mean(), knn_scores.std() * 2))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "knn_fpr, knn_tpr, _ = roc_curve(testy, knn_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill - '+var_to_predict_title)\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='KNN - '+var_to_predict_title)\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# Show title\n",
    "plt.title('KNN Classiffier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "# show the plot\n",
    "plt.savefig(var_to_predict_save+'_knn_roc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train / Cross Validation Score Plot\n",
    "skplt.estimators.plot_learning_curve(knn, trainX, trainy, cv=num_folds, title=var_to_predict_title + ' - KNN - Train / Cross Validation Score Plot')\n",
    "plt.savefig(var_to_predict_save+'_knn_txcval')\n",
    "plt.show()\n",
    "\n",
    "# Cummulative Gain Plot\n",
    "skplt.metrics.plot_cumulative_gain(testy,knn.predict_proba(testX), title=var_to_predict_title + ' - KNN - Cummulative Gains Curve')\n",
    "plt.savefig(var_to_predict_save+'_knn_cgc')\n",
    "plt.show()\n",
    "\n",
    "# Lift Curve Plot\n",
    "skplt.metrics.plot_lift_curve(testy,knn.predict_proba(testX), title=var_to_predict_title + ' - KNN - Lift Curve')\n",
    "plt.savefig(var_to_predict_save+'_knn_liftc')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve\n",
    "skplt.metrics.plot_precision_recall_curve(testy,knn.predict_proba(testX), title=var_to_predict_title + ' - KNN - Precision - Recall Curve')\n",
    "plt.savefig(var_to_predict_save+'_knn_prerecc')\n",
    "plt.show()\n",
    "\n",
    "# KS statistics Plot\n",
    "skplt.metrics.plot_ks_statistic(testy, knn.predict_proba(testX), title=var_to_predict_title+' - KNN - KS Statistics Plot')\n",
    "plt.savefig(var_to_predict_save+'_knn_ksst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion Metrics\n",
    "np.set_printoptions(precision=2)\n",
    "i = 1\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix - \"+var_to_predict_title+\", without normalization - KNN Classiffier\", None),\n",
    "                  (\"Normalized confusion matrix - \"+var_to_predict_title+\" - KNN Classiffier\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(knn, testX, testy,\n",
    "                                 display_labels=testy,\n",
    "                                 values_format='.3f',\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "    if i==1: plt.savefig(var_to_predict_save+'_knn_cm')\n",
    "    else: plt.savefig(var_to_predict_save+'_knn_cm_1')\n",
    "    i +=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART  Model - DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClasifier\n",
    "np.random.seed(42)\n",
    "\n",
    "#create a dictionary of all values we want to test\n",
    "param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(40, 65,1)\n",
    "             }\n",
    "\n",
    "# decision tree model\n",
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "#use gridsearch to test all values\n",
    "cart_gscv = GridSearchCV(cart, param_grid, cv=num_folds)\n",
    "\n",
    "#fit model to data\n",
    "cart_gscv.fit(trainX, trainy)\n",
    "print('Best parameters : ', cart_gscv.best_params_)\n",
    "print('Best Score      : ', cart_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "Based on the grid search above, the best parameter for the Decission Tree Classifier is:\n",
    "* The 1st run showed \n",
    "    * Best parameters :  {'criterion': 'gini', 'max_depth': 52}\n",
    "    * Best Score      :  0.7829467396714784\n",
    "    * As I am using an increment of 10, rerun it, using increment of 1 from +-10\n",
    "\n",
    "* Narrowing the parameters\n",
    "    * Best parameters :  {'criterion': 'gini', 'max_depth': 61}\n",
    "    * Best Score      :  0.785792268126763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling based on the outcome of the grid serach\n",
    "cart = DecisionTreeClassifier(criterion = 'gini', max_depth=61)\n",
    "cart.fit(trainX, trainy)\n",
    "\n",
    "# Predict on test set\n",
    "yhat = cart.predict(testX)\n",
    "\n",
    "# Generate a no skill prediction for ROC Curve\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# Predict probabilities\n",
    "cart_probs = cart.predict_proba(testX)\n",
    "\n",
    "# Keep probabilities for the positive outcome only\n",
    "cart_probs = cart_probs[:, 1]\n",
    "\n",
    "# Calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "cart_auc = roc_auc_score(testy, cart_probs)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "cart_scores = cross_val_score(cart, testX, testy, cv=num_folds)\n",
    "\n",
    "print(\"CART - Accuracy on training set: {:.3f}\".format(cart.score(trainX, trainy)))\n",
    "print(\"CART - Accuracy on test set: {:.3f}\".format(cart.score(testX, testy)))\n",
    "\n",
    "# Evaluate predictions\n",
    "print('CART - Precision                  : %.3f' % precision_score(testy, yhat))\n",
    "print('CART - Recall                     : %.3f' % recall_score(testy, yhat))\n",
    "print('CART - F-measure                  : %.3f' % f1_score(testy, yhat))\n",
    "print('CART - Log-Loss                   : %.3f' % log_loss(testy, yhat))\n",
    "print('CART - Cross Entropy Loss         : %.3f' % cross_entropy(yhat, testy[var_to_predict[0]]))\n",
    "print('CART - No Skill - ROC AUC         : %.3f' % (ns_auc))\n",
    "print('CART - ROC AUC                    : %.3f' % (cart_auc))\n",
    "print(\"CART - Cross Validation Accuracy  : %0.3f (+/- %0.3f)\" % (cart_scores.mean(), cart_scores.std() * 2))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "cart_fpr, cart_tpr, _ = roc_curve(testy, cart_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill - '+var_to_predict_title)\n",
    "plt.plot(cart_fpr, cart_tpr, marker='.', label='CART - '+var_to_predict_title)\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.title('CART Classiffier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "# show the plot\n",
    "plt.savefig(var_to_predict_save+'_cart_roc')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train / Cross Validation Score Plot\n",
    "skplt.estimators.plot_learning_curve(cart, trainX, trainy, cv=num_folds, title=var_to_predict_title + ' - CART - Train / Cross Validation Score Plot')\n",
    "plt.savefig(var_to_predict_save+'_cart_txcval')\n",
    "plt.show()\n",
    "\n",
    "# Cummulative Gain Plot\n",
    "skplt.metrics.plot_cumulative_gain(testy,cart.predict_proba(testX), title=var_to_predict_title + ' - CART - Cummulative Gains Curve')\n",
    "plt.savefig(var_to_predict_save+'_cart_cgc')\n",
    "plt.show()\n",
    "\n",
    "# Lift Curve Plot\n",
    "skplt.metrics.plot_lift_curve(testy,cart.predict_proba(testX), title=var_to_predict_title + ' - CART - Lift Curve')\n",
    "plt.savefig(var_to_predict_save+'_cart_liftc')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve\n",
    "skplt.metrics.plot_precision_recall_curve(testy,cart.predict_proba(testX), title=var_to_predict_title + ' - CART - Precision - Recall Curve')\n",
    "plt.savefig(var_to_predict_save+'_cart_prerecc')\n",
    "plt.show()\n",
    "\n",
    "# KS statistics Plot\n",
    "skplt.metrics.plot_ks_statistic(testy, cart.predict_proba(testX), title=var_to_predict_title+' - CART - KS Statistics Plot')\n",
    "plt.savefig(var_to_predict_save+'_cart_ksst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion Metrics\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "i = 1\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix - \"+var_to_predict_title+\", without normalization - CART\", None),\n",
    "                  (\"CART - Normalized confusion matrix - \"+var_to_predict_title, 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(cart, testX, testy,\n",
    "                                 display_labels=testy,\n",
    "                                 values_format='.3f',\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "    if i==1: plt.savefig(var_to_predict_save+'_cart_cm')\n",
    "    else: plt.savefig(var_to_predict_save+'_cart_cm_1')\n",
    "    i +=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance in trees\n",
    "feature importance rates how important each feature is for the decision a tree makes. It is a number between 0 and 1 for each feature, where 0 means “not used at all” and 1 means “perfectly predicts the target.” The feature importances always sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_df_cart = pd.DataFrame({'feature':testX.columns, 'coefs': cart.feature_importances_})\n",
    "coef_df_cart[\"abs_coeff\"] = coef_df_cart['coefs'].abs()\n",
    "coef_df_cart.sort_values(\"abs_coeff\",ascending=False,inplace=True)\n",
    "coef_df_cart.nlargest(30,columns=[\"abs_coeff\"])\n",
    "counts_cart = pd.DataFrame(coef_df_cart.nlargest(30,columns=[\"abs_coeff\"]), index=None)\n",
    "counts_cart.rename(columns={\"feature\": \"Features (MinMax Transformer)\", 'coefs':'Coefficients', 'abs_coeff': \"ABS of Coeffs.\"}, inplace=True)\n",
    "fig = ff.create_table(counts_cart, height_constant=30, index=False)\n",
    "fig.write_image(var_to_predict_save+'_cart_coef.png')\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of Coefficients\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.axhline(0, color=\"k\", clip_on=False)\n",
    "plt.ylabel(\"Coefs\",fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title('Features of CART Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "sns.barplot(y=counts_cart['Features (MinMax Transformer)'], x=counts_cart['Coefficients'], orient='h')\n",
    "plt.xticks(\n",
    "    rotation=90, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='small')\n",
    "plt.savefig(var_to_predict_save+'_cart_coef_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "#create a dictionary of all values we want to test\n",
    "param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(2, 200, 8)}\n",
    "\n",
    "# Random Forest Classifier model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#use gridsearch to test all values\n",
    "rf_gscv = GridSearchCV(rf, param_grid, cv=num_folds, scoring=scoring)\n",
    "\n",
    "#fit model to data\n",
    "rf_gscv.fit(trainX, trainy)\n",
    "print('Best parameters : ', rf_gscv.best_params_)\n",
    "print('Best score      : ', rf_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Based on the grid search above, th best parameter for the Random Forest Classifier is:\n",
    "* The 1st run showed a max_depth of 112, with gini, as I am using an increment of 10, rerun it, using increment of 1 from +-10\n",
    "    * Best parameters :  {'criterion': 'gini', 'max_depth': 42}\n",
    "    * Best score      :  0.8541958041958042\n",
    "* Narrowing the search around the best parameter of the 1st search\n",
    "    * Best parameters :  {'criterion': 'entropy', 'max_depth': 32}\n",
    "    * Best score      :  0.8545454545454545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling based on the outcome of the grid serach of the random forest\n",
    "rf = DecisionTreeClassifier(criterion = 'gini', max_depth=130)\n",
    "rf.fit(trainX, trainy)\n",
    "\n",
    "# Predict on test set\n",
    "yhat = rf.predict(testX)\n",
    "\n",
    "# Generate a no skill prediction for ROC Curve\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# Predict probabilities\n",
    "rf_probs = rf.predict_proba(testX)\n",
    "\n",
    "# Keep probabilities for the positive outcome only\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "# Calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "rf_auc = roc_auc_score(testy, rf_probs)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "rf_scores = cross_val_score(rf, testX, testy, cv=num_folds)\n",
    "\n",
    "print(\"Random Forest - Accuracy on training set: {:.3f}\".format(rf.score(trainX, trainy)))\n",
    "print(\"Random Forest - Accuracy on test set: {:.3f}\".format(rf.score(testX, testy)))\n",
    "\n",
    "# Evaluate predictions\n",
    "#print('Accuracy                   : %.3f' % accuracy_score(testy, yhat))\n",
    "print('Random Forest - Precision                  : %.3f' % precision_score(testy, yhat))\n",
    "print('Random Forest - Recall                     : %.3f' % recall_score(testy, yhat))\n",
    "print('Random Forest - F-measure                  : %.3f' % f1_score(testy, yhat))\n",
    "print('Random Forest - Log-Loss                   : %.3f' % log_loss(testy, yhat))\n",
    "print('Random Forest - Cross Entropy Loss         : %.3f' % cross_entropy(yhat, testy[var_to_predict[0]]))\n",
    "print('Random Forest - No Skill - ROC AUC         : %.3f' % (ns_auc))\n",
    "print('Random Forest - ROC AUC                    : %.3f' % (rf_auc))\n",
    "print(\"Random Forest - Cross Validation Accuracy  : %0.3f (+/- %0.3f)\" % (rf_scores.mean(), rf_scores.std() * 2))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(testy, rf_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill - '+var_to_predict_title)\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest - '+var_to_predict_title)\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.title('Random Forest - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "# show the plot\n",
    "plt.savefig(var_to_predict_save+'_rf_roc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train / Cross Validation Score Plot\n",
    "skplt.estimators.plot_learning_curve(rf, trainX, trainy, cv=num_folds, title=var_to_predict_title + ' - Random Forest - Train / Cross Validation Score Plot')\n",
    "plt.savefig(var_to_predict_save+'_rf_txcval')\n",
    "plt.show()\n",
    "\n",
    "# Cummulative Gain Plot\n",
    "skplt.metrics.plot_cumulative_gain(testy,rf.predict_proba(testX), title=var_to_predict_title + ' - Random Forest - Cummulative Gains Curve')\n",
    "plt.savefig(var_to_predict_save+'_rf_cgc')\n",
    "plt.show()\n",
    "\n",
    "# Lift Curve Plot\n",
    "skplt.metrics.plot_lift_curve(testy,rf.predict_proba(testX), title=var_to_predict_title + ' - Random Forest - Lift Curve')\n",
    "plt.savefig(var_to_predict_save+'_rf_liftc')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve\n",
    "skplt.metrics.plot_precision_recall_curve(testy,rf.predict_proba(testX), title=var_to_predict_title + ' - Random Forest - Precision - Recall Curve')\n",
    "plt.savefig(var_to_predict_save+'_rf_prerecc')\n",
    "plt.show()\n",
    "\n",
    "# KS statistics Plot\n",
    "skplt.metrics.plot_ks_statistic(testy, rf.predict_proba(testX), title=var_to_predict_title+' - Random Forest - KS Statistics Plot')\n",
    "plt.savefig(var_to_predict_save+'_rf_ksst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion Metrics\n",
    "np.set_printoptions(precision=2)\n",
    "i = 1 \n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix - \"+var_to_predict_title+ \", without normalization - Random Forest\", None),\n",
    "                  (\"Normalized confusion matrix - \"+var_to_predict_title+\" - Random Forest\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(rf, testX, testy,\n",
    "                                 display_labels=testy,\n",
    "                                 values_format='.3f',\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "    if i==1: plt.savefig(var_to_predict_save+'_rf_cm')\n",
    "    else: plt.savefig(var_to_predict_save+'_rf_cm_1')\n",
    "    i +=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "feature importance rates how important each feature is for the random forest tree makes. It is a number between 0 and 1 for each feature, where 0 means “not used at all” and 1 means “perfectly predicts the target.” The feature importances always sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_df_rf = pd.DataFrame({'feature':testX.columns, 'coefs': rf.feature_importances_})\n",
    "coef_df_rf[\"abs_coeff\"] = coef_df_rf['coefs'].abs()\n",
    "coef_df_rf.sort_values(\"abs_coeff\",ascending=False,inplace=True)\n",
    "coef_df_rf.nlargest(30,columns=[\"abs_coeff\"])\n",
    "counts_rf = pd.DataFrame(coef_df_rf.nlargest(30,columns=[\"abs_coeff\"]), index=None)\n",
    "counts_rf.rename(columns={\"feature\": \"Features (MinMax Transformer)\", 'coefs':'Coefficients', 'abs_coeff': \"ABS of Coeffs.\"}, inplace=True)\n",
    "fig = ff.create_table(counts_rf, height_constant=30, index=False)\n",
    "fig.write_image(var_to_predict_save+'_rf_coef.png')\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of Coefficients\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.axhline(0, color=\"k\", clip_on=False)\n",
    "plt.ylabel(\"Coefs\",fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title('Features of Random Forest Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "sns.barplot(y=counts_rf['Features (MinMax Transformer)'], x=counts_rf['Coefficients'], orient='h')\n",
    "plt.xticks(\n",
    "    rotation=90, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='small')\n",
    "plt.savefig(var_to_predict_save+'_rf_coef_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#create a dictionary of all values we want to test\n",
    "param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(2, 200,10), 'n_estimators': np.arange(10, 200, 10)}\n",
    "\n",
    "# Random Forest Classifier model\n",
    "et = ExtraTreesClassifier()\n",
    "\n",
    "#use gridsearch to test all values\n",
    "et_gscv = GridSearchCV(et, param_grid, cv=num_folds, scoring=scoring)\n",
    "\n",
    "#fit model to data\n",
    "et_gscv.fit(trainX, trainy)\n",
    "print('Best parameters : ', et_gscv.best_params_)\n",
    "print('Best score      : ', et_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Based on the grid search above, th best parameter for the Random Forest Classifier is:\n",
    "* The 1st run showed a max_depth of 32, with gini, as I am using an increment of 10, rerun it, using increment of 1 from +-10\n",
    "    * Best parameters :  {'criterion': 'gini', 'max_depth': 92, 'n_estimators': 120}\n",
    "    * Best score      :  0.8388485150157624\n",
    "* Narrowing the search around the best parameter of the 1st search\n",
    "    * Best parameters :  {'criterion': 'entropy', 'max_depth': 42}\n",
    "    * Best score      :  0.855944055944056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling based on the outcome of the grid serach of the random forest\n",
    "et = ExtraTreesClassifier(criterion = 'gini', max_depth=92, n_estimators=120)\n",
    "et.fit(trainX, trainy)\n",
    "\n",
    "# Predict on test set\n",
    "yhat = et.predict(testX)\n",
    "\n",
    "# Generate a no skill prediction for ROC Curve\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# Predict probabilities\n",
    "et_probs = et.predict_proba(testX)\n",
    "\n",
    "# Keep probabilities for the positive outcome only\n",
    "et_probs = et_probs[:, 1]\n",
    "\n",
    "# Calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "et_auc = roc_auc_score(testy, et_probs)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "scores_et = cross_val_score(et, testX, testy, cv=num_folds)\n",
    "\n",
    "print(\"Extra Tree Classifier - Accuracy on training set: {:.3f}\".format(et.score(trainX, trainy)))\n",
    "print(\"Extra Tree Classifier - Accuracy on test set: {:.3f}\".format(et.score(testX, testy)))\n",
    "\n",
    "# Evaluate predictions\n",
    "#print('Accuracy                   : %.3f' % accuracy_score(testy, yhat))\n",
    "print('Extra Tree Classifier - Precision                  : %.3f' % precision_score(testy, yhat))\n",
    "print('Extra Tree Classifier - Recall                     : %.3f' % recall_score(testy, yhat))\n",
    "print('Extra Tree Classifier - F-measure                  : %.3f' % f1_score(testy, yhat))\n",
    "print('Extra Tree Classifier - Log-Loss                   : %.3f' % log_loss(testy, yhat))\n",
    "print('Extra Tree Classifier - Cross Entropy Loss         : %.3f' % cross_entropy(yhat, testy[var_to_predict[0]]))\n",
    "print('Extra Tree Classifier - No Skill - ROC AUC         : %.3f' % (ns_auc))\n",
    "print('Extra Tree Classifier - ROC AUC                    : %.3f' % (et_auc))\n",
    "print(\"Extra Tree Classifier - Cross Validation Accuracy  : %0.3f (+/- %0.3f)\" % (scores_et.mean(), scores_et.std() * 2))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "et_fpr, et_tpr, _ = roc_curve(testy, et_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill - '+var_to_predict_title)\n",
    "plt.plot(et_fpr, et_tpr, marker='.', label='Extra Tree Classifier - '+var_to_predict_title)\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.title('Extra Tree Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "# show the plot\n",
    "plt.savefig(var_to_predict_save+'_et_roc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train / Cross Validation Score Plot\n",
    "skplt.estimators.plot_learning_curve(et, trainX, trainy, cv=num_folds, title=var_to_predict_title + ' - Extra Tree Classifier - Train / Cross Validation Score Plot')\n",
    "plt.savefig(var_to_predict_save+'_et_txcval')\n",
    "plt.show()\n",
    "\n",
    "# Cummulative Gain Plot\n",
    "skplt.metrics.plot_cumulative_gain(testy,et.predict_proba(testX), title=var_to_predict_title + ' - Extra Tree Classifier - Cummulative Gains Curve')\n",
    "plt.savefig(var_to_predict_save+'_et_cgc')\n",
    "plt.show()\n",
    "\n",
    "# Lift Curve Plot\n",
    "skplt.metrics.plot_lift_curve(testy,et.predict_proba(testX), title=var_to_predict_title + ' - Extra Tree Classifier - Lift Curve')\n",
    "plt.savefig(var_to_predict_save+'_et_liftc')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve\n",
    "skplt.metrics.plot_precision_recall_curve(testy,et.predict_proba(testX), title=var_to_predict_title + ' - Extra Tree Classifier - Precision - Recall Curve')\n",
    "plt.savefig(var_to_predict_save+'_et_prerecc')\n",
    "plt.show()\n",
    "\n",
    "# KS statistics Plot\n",
    "skplt.metrics.plot_ks_statistic(testy, et.predict_proba(testX), title=var_to_predict_title+' - Extra Tree Classifier - KS Statistics Plot')\n",
    "plt.savefig(var_to_predict_save+'_et_ksst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Metrics\n",
    "np.set_printoptions(precision=2)\n",
    "i = 1\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix - \"+var_to_predict_title+\", without normalization - Extra Tree Classifier\", None),\n",
    "                  (\"Extra Tree Classifier - Normalized confusion matrix - \"+var_to_predict_title, 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(et, testX, testy,\n",
    "                                 display_labels=testy,\n",
    "                                 values_format='.3f',\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "    if i==1: plt.savefig(var_to_predict_save+'_et_cm')\n",
    "    else: plt.savefig(var_to_predict_save+'_et_cm_1')\n",
    "    i +=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_df_et = pd.DataFrame({'feature':testX.columns, 'coefs': et.feature_importances_})\n",
    "coef_df_et[\"abs_coeff\"] = coef_df_et['coefs'].abs()\n",
    "coef_df_et.sort_values(\"abs_coeff\",ascending=False,inplace=True)\n",
    "coef_df_et.nlargest(30,columns=[\"abs_coeff\"])\n",
    "counts_et = pd.DataFrame(coef_df_et.nlargest(30,columns=[\"abs_coeff\"]), index=None)\n",
    "counts_et.rename(columns={\"feature\": \"Features (MinMax Transformer)\", 'coefs':'Coefficients', 'abs_coeff': \"ABS of Coeffs.\"}, inplace=True)\n",
    "fig = ff.create_table(counts_et, height_constant=30, index=False)\n",
    "fig.write_image(var_to_predict_save+'_et_coef.png')\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cumsum = coef_df_et.copy()\n",
    "df_cumsum.set_index('feature', inplace=True)\n",
    "df_cumsum.abs_coeff.cumsum().plot(kind='barh', figsize=(12,10))\n",
    "plt.xticks(\n",
    "            rotation=90, \n",
    "            horizontalalignment='right',\n",
    "            fontweight='light',\n",
    "            fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of Coefficients\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.axhline(0, color=\"k\", clip_on=False)\n",
    "plt.ylabel(\"Coefs\",fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title('Features of Extra Tree Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "sns.barplot(y=counts_et['Features (MinMax Transformer)'], x=counts_et['Coefficients'], orient='h')\n",
    "plt.xticks(\n",
    "    rotation=90, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='small')\n",
    "plt.savefig(var_to_predict_save+'_et_coef_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOSTING Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "scoring = 'accuracy'\n",
    "\n",
    "#create a dictionary of all values we want to test\n",
    "param_grid = {'max_depth': [5,6,7, 8], 'n_estimators': range(50, 250, 50), \n",
    "              'learning_rate': [0.1, 0.01, 0.05], 'min_child_weight':[2,4, 6],\n",
    "              'gamma':[0.1]\n",
    "             }\n",
    "# CV model\n",
    "xgb = XGBClassifier(objective= 'binary:logistic',nthread=4)\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "xgb_gscv = GridSearchCV(estimator=xgb,param_grid=param_grid, scoring = scoring, \n",
    "                        n_jobs = 10, cv = kfold,verbose=True\n",
    "                       )\n",
    "xgb_gscv.fit(trainX, trainy)\n",
    "print('Best parameters - : ', xgb_gscv.best_params_)\n",
    "print('Best score      : ', xgb_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Best parameters - :  {'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_weight': 2, 'n_estimators': 200}\n",
    "Best score      :  0.9773189547971137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling based on the outcome of the grid serach of the random forest\n",
    "xgb = XGBClassifier(objective= 'binary:logistic',nthread=4, gamma=0.1,\n",
    "                          learning_rate=0.1, max_depth=8, min_child_weight=2,\n",
    "                          n_estimators=200                        \n",
    "                         )\n",
    "xgb.fit(trainX, trainy)\n",
    "\n",
    "# Predict on test set\n",
    "yhat = xgb.predict(testX)\n",
    "\n",
    "# Generate a no skill prediction for ROC Curve\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# Predict probabilities\n",
    "xgb_probs = xgb.predict_proba(testX)\n",
    "\n",
    "# Keep probabilities for the positive outcome only\n",
    "xgb_probs = xgb_probs[:, 1]\n",
    "\n",
    "# Calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "xgb_auc = roc_auc_score(testy, xgb_probs)\n",
    "\n",
    "# Calculate Cross valdation scores\n",
    "scores_xgb = cross_val_score(xgb, testX, testy, cv=num_folds)\n",
    "\n",
    "print(\"XGBoost Classifier - Accuracy on training set: {:.3f}\".format(xgb.score(trainX, trainy)))\n",
    "print(\"XGBoost Classifier - Accuracy on test set: {:.3f}\".format(xgb.score(testX, testy)))\n",
    "\n",
    "# Evaluate predictions\n",
    "#print('Accuracy                   : %.3f' % accuracy_score(testy, yhat))\n",
    "print('XGBoost Classifier - Precision                  : %.3f' % precision_score(testy, yhat))\n",
    "print('XGBoost Classifier - Recall                     : %.3f' % recall_score(testy, yhat))\n",
    "print('XGBoost Classifier - F-measure                  : %.3f' % f1_score(testy, yhat))\n",
    "print('XGBoost Classifier - Log-Loss                   : %.3f' % log_loss(testy, yhat))\n",
    "print('XGBoost Classifier - Cross Entropy Loss         : %.3f' % cross_entropy(yhat, testy[var_to_predict[0]]))\n",
    "print('XGBoost Classifier - No Skill - ROC AUC         : %.3f' % (ns_auc))\n",
    "print('XGBoost Classifier - ROC AUC                    : %.3f' % (xgb_auc))\n",
    "print(\"XGBoost Classifier - Cross Validation Accuracy  : %0.3f (+/- %0.3f)\" % (scores_xgb.mean(), scores_xgb.std() * 2))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(testy, xgb_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill - '+var_to_predict_title)\n",
    "plt.plot(xgb_fpr, xgb_tpr, marker='.', label='XGBoost Classifier - '+var_to_predict_title)\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.title('XGBooster Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "# show the plot\n",
    "plt.savefig(var_to_predict_save+'_xgb_roc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train / Cross Validation Score Plot\n",
    "skplt.estimators.plot_learning_curve(xgb, trainX, trainy, cv=num_folds, title=var_to_predict_title + ' - XGBoost Classifier - Train / Cross Validation Score Plot')\n",
    "plt.savefig(var_to_predict_save+'_xgb_txcval')\n",
    "plt.show()\n",
    "\n",
    "# Cummulative Gain Plot\n",
    "skplt.metrics.plot_cumulative_gain(testy,xgb.predict_proba(testX), title=var_to_predict_title + ' - XGBoost Classifier - Cummulative Gains Curve')\n",
    "plt.savefig(var_to_predict_save+'_xgb_cgc')\n",
    "plt.show()\n",
    "\n",
    "# Lift Curve Plot\n",
    "skplt.metrics.plot_lift_curve(testy,xgb.predict_proba(testX), title=var_to_predict_title + ' - XGBoost Classifier - Lift Curve')\n",
    "plt.savefig(var_to_predict_save+'_xgb_liftc')\n",
    "plt.show()\n",
    "\n",
    "# Precision - Recall Curve\n",
    "skplt.metrics.plot_precision_recall_curve(testy,xgb.predict_proba(testX), title=var_to_predict_title + ' - XGBoost Classifier - Precision - Recall Curve')\n",
    "plt.savefig(var_to_predict_save+'_xgb_prerecc')\n",
    "plt.show()\n",
    "\n",
    "# KS statistics Plot\n",
    "skplt.metrics.plot_ks_statistic(testy, xgb.predict_proba(testX), title=var_to_predict_title+' - XGBoost Classifier - KS Statistics Plot')\n",
    "plt.savefig(var_to_predict_save+'_xgb_ksst')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_df_xgb = pd.DataFrame({'feature':testX.columns, 'coefs': xgb.feature_importances_})\n",
    "coef_df_xgb[\"abs_coeff\"] = coef_df_xgb['coefs'].abs()\n",
    "coef_df_xgb.sort_values(\"abs_coeff\",ascending=False,inplace=True)\n",
    "coef_df_xgb.nlargest(30,columns=[\"abs_coeff\"])\n",
    "counts_xgb = pd.DataFrame(coef_df_xgb.nlargest(30,columns=[\"abs_coeff\"]), index=None)\n",
    "counts_xgb.rename(columns={\"feature\": \"Features (MinMax Transformer)\", 'coefs':'Coefficients', 'abs_coeff': \"ABS of Coeffs.\"}, inplace=True)\n",
    "fig = ff.create_table(counts_xgb, height_constant=30, index=False)\n",
    "fig.write_image(var_to_predict_save+'_xgb_coef.png')\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of Coefficients\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.axhline(0, color=\"k\", clip_on=False)\n",
    "plt.ylabel(\"Coefs\",fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title('Features of XGBooster Classifier - '+var_to_predict_title,loc='center', fontdict={'fontsize':14, 'fontweight':'bold'})\n",
    "sns.barplot(y=counts_xgb['Features (MinMax Transformer)'], x=counts_xgb['Coefficients'], orient='h')\n",
    "plt.xticks(\n",
    "    rotation=90, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='small')\n",
    "plt.savefig(var_to_predict_save+'_xgb_coef_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras Classifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "# Create input and output dataframes\n",
    "\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# create model\n",
    "keras_nn = Sequential()\n",
    "keras_nn.add(Dense(trainX.shape[1], input_dim=trainX.shape[1], activation='relu'))\n",
    "keras_nn.add(Dropout(0.2))\n",
    "keras_nn.add(Dense(int(trainX.shape[1]/2), activation='relu'))\n",
    "keras_nn.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.01, momentum=0.8)\n",
    "keras_nn.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[scoring])\n",
    "# Fit the model\n",
    "history = keras_nn.fit(trainX, trainy, validation_data=(testX,testy), epochs=150, batch_size=10, verbose=1)\n",
    "# evaluate the model\n",
    "scores = keras_nn.evaluate(trainX, trainy, verbose=0)\n",
    "print(\"Keras - Accuracy Train Set : %.3f\" % scores[1])\n",
    "scores = keras_nn.evaluate(testX, testy, verbose=0)\n",
    "print(\"Keras - Accuracy Test Set  : %.3f\" % scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Keras Model accuracy - '+var_to_predict_title )\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(var_to_predict_save+'_keras_model_1')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Keras Model Loss - '+ var_to_predict_title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(var_to_predict_save+'_keras_model_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used from this point below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM grid search evaluation\n",
    "\n",
    "np.random.seed(42)\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0, 2.3, 2.5, 2.7, 3.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "svc = SVC(gamma='auto')\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "grid = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=scoring, cv=kfold, iid=True)\n",
    "grid_result = grid.fit(trainX, trainy)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale fields using Standard Scaler\n",
    "\n",
    "features_to_scale = ['aaa_mean_age', 'length_of_residence', 'aaa_mean_child', \n",
    "                     'total_calls', 'total_member_cost', 'total_cost'\n",
    "                    ]\n",
    "df = pd.DataFrame(df_test_over, columns=features_to_scale)\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df_std = scaler.fit_transform(df)\n",
    "scaled_df_std = pd.DataFrame(scaled_df_std, columns=features_to_scale)\n",
    "\n",
    "\n",
    "for col in features_to_scale:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "    \n",
    "    ax1.set_title('Before Scaling Standard Scaler')\n",
    "    ax2.set_title('After Standard Scaler')\n",
    "\n",
    "    sns.kdeplot(df[col], ax=ax1)\n",
    "    sns.kdeplot(scaled_df_std[col], ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_std.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale fields using Min-Max scaler\n",
    "\n",
    "features_to_scale = ['aaa_mean_age', 'length_of_residence', 'aaa_mean_child', \n",
    "                     'total_calls', 'total_member_cost', 'total_cost'\n",
    "                    ]\n",
    "df = pd.DataFrame(df_test_over, columns=features_to_scale)\n",
    "\n",
    "# Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df_mm = scaler.fit_transform(df)\n",
    "scaled_df_mm = pd.DataFrame(scaled_df_mm, columns=features_to_scale)\n",
    "\n",
    "for col in features_to_scale:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "    ax1.set_title('Before Scaling Min-Max')\n",
    "    sns.kdeplot(df[col], ax=ax1)\n",
    "    ax2.set_title('After Min-Max Scaling')\n",
    "    sns.kdeplot(scaled_df_mm[col], ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_mm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale fields using Robust Scaler\n",
    "\n",
    "features_to_scale = ['aaa_mean_age', 'length_of_residence', 'aaa_mean_child', \n",
    "                     'total_calls', 'total_member_cost', 'total_cost'\n",
    "                    ]\n",
    "df = pd.DataFrame(df_test_over, columns=features_to_scale)\n",
    "\n",
    "# Robust Scaler- excellent with outliers\n",
    "scaler = RobustScaler()\n",
    "scaled_df_rs = scaler.fit_transform(df)\n",
    "scaled_df_rs = pd.DataFrame(scaled_df_rs, columns=features_to_scale)\n",
    "for col in features_to_scale:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "    ax1.set_title('Before Scaling Robust Scaling')\n",
    "    sns.kdeplot(df[col], ax=ax1)\n",
    "    ax2.set_title('After Robust Scaling')\n",
    "    sns.kdeplot(scaled_df_rs[col], ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_rs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale fields using Normalizer\n",
    "\n",
    "features_to_scale = ['aaa_mean_age', 'length_of_residence', 'aaa_mean_child', \n",
    "                     'total_calls', 'total_member_cost', 'total_cost'\n",
    "                    ]\n",
    "df = pd.DataFrame(df_test_over, columns=features_to_scale)\n",
    "\n",
    "# Normalizer\n",
    "scaler = Normalizer()\n",
    "scaled_df_nm = scaler.fit_transform(df)\n",
    "scaled_df_nm = pd.DataFrame(scaled_df_nm, columns=features_to_scale)\n",
    "for col in features_to_scale:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "    ax1.set_title('Before Scaling Normalizer')\n",
    "    sns.kdeplot(df[col], ax=ax1)\n",
    "    ax2.set_title('After Normalizer')\n",
    "    sns.kdeplot(scaled_df_nm[col], ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_nm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale fields using Quantile Transformer\n",
    "\n",
    "features_to_scale = ['aaa_mean_age', 'length_of_residence', 'aaa_mean_child', \n",
    "                     'total_calls', 'total_member_cost', 'total_cost'\n",
    "                    ]\n",
    "df = pd.DataFrame(df_test_over, columns=features_to_scale)\n",
    "\n",
    "# QuantileTransformer\n",
    "scaler = QuantileTransformer(n_quantiles=200, output_distribution='normal')\n",
    "scaled_df_qt = scaler.fit_transform(df)\n",
    "scaled_df_qt = pd.DataFrame(scaled_df_qt, columns=features_to_scale)\n",
    "for col in features_to_scale:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "    ax1.set_title('Before Scaling QuantileTransformer')\n",
    "    sns.kdeplot(df[col], ax=ax1)\n",
    "    ax2.set_title('After QuantileTransformer')\n",
    "    sns.kdeplot(scaled_df_nm[col], ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_qt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale fields using Power Transformer\n",
    "\n",
    "features_to_scale = ['aaa_mean_age', 'length_of_residence', 'aaa_mean_child', \n",
    "                     'total_calls', 'total_member_cost', 'total_cost'\n",
    "                    ]\n",
    "df = pd.DataFrame(df_test_over, columns=features_to_scale)\n",
    "\n",
    "# PowerTransformer\n",
    "scaler = PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "scaled_df_pt = scaler.fit_transform(df)\n",
    "scaled_df_pt = pd.DataFrame(scaled_df_pt, columns=features_to_scale)\n",
    "for col in features_to_scale:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))\n",
    "    ax1.set_title('Before Scaling PowerTransformer')\n",
    "    sns.kdeplot(df[col], ax=ax1)\n",
    "    ax2.set_title('After PowerTransformer')\n",
    "    sns.kdeplot(scaled_df_nm[col], ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_pt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_test_over[var_to_predict[0]].values)\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('y value')\n",
    "plt.title('Change in y value over the data set')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
